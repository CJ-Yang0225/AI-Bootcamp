{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch2 regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWjB11xxaJ4+UXl3IyUYLP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohAgOgSNnyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kp2ZmAyN4qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 超參數\n",
        "batch_n = 100\n",
        "in_features = 3\n",
        "out_feature = 1\n",
        "epoch_n = 100\n",
        "leaning_rate = 0.01 #越大收斂越快"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiokQjgeOqCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8b1f74b-5ee2-45a9-f5a5-21ed2013b9ae"
      },
      "source": [
        "x = torch.rand(batch_n, in_features) # 0～1的浮點數\n",
        "coef = torch.Tensor([[3.],[5.],[1.]]) # 第二個最集中，結果為100x1的矩陣，\n",
        "y = x.mm(coef) # 矩陣相乘\n",
        "y = y.add(torch.rand(batch_n, out_feature)) # 加入雜訊\n",
        "y"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.3171],\n",
              "        [3.0751],\n",
              "        [3.8242],\n",
              "        [7.8334],\n",
              "        [7.0342],\n",
              "        [3.5945],\n",
              "        [2.2809],\n",
              "        [5.8077],\n",
              "        [6.6986],\n",
              "        [1.5054],\n",
              "        [6.3461],\n",
              "        [2.7473],\n",
              "        [2.8408],\n",
              "        [5.9754],\n",
              "        [5.6649],\n",
              "        [2.8392],\n",
              "        [2.2433],\n",
              "        [5.2889],\n",
              "        [5.7765],\n",
              "        [8.2019],\n",
              "        [7.5252],\n",
              "        [5.9206],\n",
              "        [4.3393],\n",
              "        [5.1510],\n",
              "        [5.3437],\n",
              "        [5.6169],\n",
              "        [6.7861],\n",
              "        [4.7338],\n",
              "        [3.0712],\n",
              "        [6.6043],\n",
              "        [2.9412],\n",
              "        [3.2240],\n",
              "        [3.3231],\n",
              "        [2.0454],\n",
              "        [6.5138],\n",
              "        [1.5250],\n",
              "        [4.5049],\n",
              "        [3.6134],\n",
              "        [3.1933],\n",
              "        [6.0264],\n",
              "        [3.6001],\n",
              "        [4.8950],\n",
              "        [3.8473],\n",
              "        [5.0339],\n",
              "        [4.1727],\n",
              "        [5.0929],\n",
              "        [3.9264],\n",
              "        [4.8174],\n",
              "        [6.0744],\n",
              "        [3.2797],\n",
              "        [5.9045],\n",
              "        [6.9821],\n",
              "        [3.3662],\n",
              "        [4.7169],\n",
              "        [2.8470],\n",
              "        [1.6907],\n",
              "        [4.1501],\n",
              "        [5.7764],\n",
              "        [6.3398],\n",
              "        [4.6896],\n",
              "        [5.3821],\n",
              "        [5.4334],\n",
              "        [6.7474],\n",
              "        [4.0475],\n",
              "        [5.9403],\n",
              "        [5.5915],\n",
              "        [4.6364],\n",
              "        [4.8586],\n",
              "        [5.6483],\n",
              "        [5.1576],\n",
              "        [2.5955],\n",
              "        [2.4187],\n",
              "        [5.7551],\n",
              "        [6.4137],\n",
              "        [6.7858],\n",
              "        [2.9605],\n",
              "        [7.8058],\n",
              "        [1.5226],\n",
              "        [5.6173],\n",
              "        [6.6643],\n",
              "        [5.5688],\n",
              "        [8.0008],\n",
              "        [5.2615],\n",
              "        [5.7739],\n",
              "        [5.6920],\n",
              "        [6.4087],\n",
              "        [5.2430],\n",
              "        [7.2106],\n",
              "        [8.9127],\n",
              "        [5.0109],\n",
              "        [4.2963],\n",
              "        [7.8866],\n",
              "        [4.0886],\n",
              "        [4.4011],\n",
              "        [5.2875],\n",
              "        [5.5950],\n",
              "        [3.4527],\n",
              "        [3.5695],\n",
              "        [5.0142],\n",
              "        [5.8442]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WltThG7iQVx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7_eyQqjQmwk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2c1b16ba-7946-4002-f1e0-b78f442f78a3"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x[:,0],y) # 散布圖，取全部row\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVtQ5OtnRVXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8ef38984-a1fa-4ac6-8873-0d6138c5355b"
      },
      "source": [
        "w = torch.rand(in_features, out_feature) # weight,學習讓w接近coef的3,5,1\n",
        "w"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1275],\n",
              "        [0.2582],\n",
              "        [0.7285]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kr7ZUfJSs6M",
        "colab_type": "text"
      },
      "source": [
        "# 土法煉鋼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmAGRQCzTGaH",
        "colab_type": "text"
      },
      "source": [
        "MSE\n",
        "\n",
        "1.   1/n Summation(y-hat - y)^2\n",
        "2.   y-hat = w * x\n",
        "3.   1/n Summation(wx - y)^2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqkmVQ2aSyOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ca85ea7-5af9-4ded-b0af-323092a6da79"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_predict = x.mm(w) # y-hat\n",
        "  loss = (y_predict - y).pow(2).mean()\n",
        "  print(\"Epoch: {}, Loss: {:.4f}\".format(epoch,loss))\n",
        "  grad_y_pred = 2 * (y_predict - y)\n",
        "  grad_w = x.t().mm(grad_y_pred)\n",
        "  w -= leaning_rate * grad_w\n",
        "w"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 21.0711\n",
            "Epoch: 1, Loss: 11.4451\n",
            "Epoch: 2, Loss: 6.2702\n",
            "Epoch: 3, Loss: 3.4802\n",
            "Epoch: 4, Loss: 1.9698\n",
            "Epoch: 5, Loss: 1.1477\n",
            "Epoch: 6, Loss: 0.6967\n",
            "Epoch: 7, Loss: 0.4469\n",
            "Epoch: 8, Loss: 0.3065\n",
            "Epoch: 9, Loss: 0.2264\n",
            "Epoch: 10, Loss: 0.1796\n",
            "Epoch: 11, Loss: 0.1516\n",
            "Epoch: 12, Loss: 0.1344\n",
            "Epoch: 13, Loss: 0.1234\n",
            "Epoch: 14, Loss: 0.1163\n",
            "Epoch: 15, Loss: 0.1115\n",
            "Epoch: 16, Loss: 0.1081\n",
            "Epoch: 17, Loss: 0.1058\n",
            "Epoch: 18, Loss: 0.1041\n",
            "Epoch: 19, Loss: 0.1028\n",
            "Epoch: 20, Loss: 0.1019\n",
            "Epoch: 21, Loss: 0.1012\n",
            "Epoch: 22, Loss: 0.1007\n",
            "Epoch: 23, Loss: 0.1004\n",
            "Epoch: 24, Loss: 0.1001\n",
            "Epoch: 25, Loss: 0.0998\n",
            "Epoch: 26, Loss: 0.0997\n",
            "Epoch: 27, Loss: 0.0996\n",
            "Epoch: 28, Loss: 0.0995\n",
            "Epoch: 29, Loss: 0.0994\n",
            "Epoch: 30, Loss: 0.0993\n",
            "Epoch: 31, Loss: 0.0993\n",
            "Epoch: 32, Loss: 0.0993\n",
            "Epoch: 33, Loss: 0.0992\n",
            "Epoch: 34, Loss: 0.0992\n",
            "Epoch: 35, Loss: 0.0992\n",
            "Epoch: 36, Loss: 0.0992\n",
            "Epoch: 37, Loss: 0.0992\n",
            "Epoch: 38, Loss: 0.0992\n",
            "Epoch: 39, Loss: 0.0992\n",
            "Epoch: 40, Loss: 0.0992\n",
            "Epoch: 41, Loss: 0.0992\n",
            "Epoch: 42, Loss: 0.0992\n",
            "Epoch: 43, Loss: 0.0992\n",
            "Epoch: 44, Loss: 0.0992\n",
            "Epoch: 45, Loss: 0.0992\n",
            "Epoch: 46, Loss: 0.0992\n",
            "Epoch: 47, Loss: 0.0992\n",
            "Epoch: 48, Loss: 0.0992\n",
            "Epoch: 49, Loss: 0.0992\n",
            "Epoch: 50, Loss: 0.0992\n",
            "Epoch: 51, Loss: 0.0992\n",
            "Epoch: 52, Loss: 0.0992\n",
            "Epoch: 53, Loss: 0.0992\n",
            "Epoch: 54, Loss: 0.0992\n",
            "Epoch: 55, Loss: 0.0992\n",
            "Epoch: 56, Loss: 0.0992\n",
            "Epoch: 57, Loss: 0.0992\n",
            "Epoch: 58, Loss: 0.0992\n",
            "Epoch: 59, Loss: 0.0992\n",
            "Epoch: 60, Loss: 0.0992\n",
            "Epoch: 61, Loss: 0.0992\n",
            "Epoch: 62, Loss: 0.0992\n",
            "Epoch: 63, Loss: 0.0992\n",
            "Epoch: 64, Loss: 0.0992\n",
            "Epoch: 65, Loss: 0.0992\n",
            "Epoch: 66, Loss: 0.0992\n",
            "Epoch: 67, Loss: 0.0992\n",
            "Epoch: 68, Loss: 0.0992\n",
            "Epoch: 69, Loss: 0.0992\n",
            "Epoch: 70, Loss: 0.0992\n",
            "Epoch: 71, Loss: 0.0992\n",
            "Epoch: 72, Loss: 0.0992\n",
            "Epoch: 73, Loss: 0.0992\n",
            "Epoch: 74, Loss: 0.0992\n",
            "Epoch: 75, Loss: 0.0992\n",
            "Epoch: 76, Loss: 0.0992\n",
            "Epoch: 77, Loss: 0.0992\n",
            "Epoch: 78, Loss: 0.0992\n",
            "Epoch: 79, Loss: 0.0992\n",
            "Epoch: 80, Loss: 0.0992\n",
            "Epoch: 81, Loss: 0.0992\n",
            "Epoch: 82, Loss: 0.0992\n",
            "Epoch: 83, Loss: 0.0992\n",
            "Epoch: 84, Loss: 0.0992\n",
            "Epoch: 85, Loss: 0.0992\n",
            "Epoch: 86, Loss: 0.0992\n",
            "Epoch: 87, Loss: 0.0992\n",
            "Epoch: 88, Loss: 0.0992\n",
            "Epoch: 89, Loss: 0.0992\n",
            "Epoch: 90, Loss: 0.0992\n",
            "Epoch: 91, Loss: 0.0992\n",
            "Epoch: 92, Loss: 0.0992\n",
            "Epoch: 93, Loss: 0.0992\n",
            "Epoch: 94, Loss: 0.0992\n",
            "Epoch: 95, Loss: 0.0992\n",
            "Epoch: 96, Loss: 0.0992\n",
            "Epoch: 97, Loss: 0.0992\n",
            "Epoch: 98, Loss: 0.0992\n",
            "Epoch: 99, Loss: 0.0992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.2145],\n",
              "        [5.1359],\n",
              "        [1.5014]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDsOlw8qWRf7",
        "colab_type": "text"
      },
      "source": [
        "1. (y-hat - y)^2 -> (f(w) - y)^2\n",
        "2. g( (f(w) - y)^2 )\n",
        "3. Chain Rule: g(f(w))/dw -> 2(y-hat - y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSI2dssSY5lM",
        "colab_type": "text"
      },
      "source": [
        "##Auto Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpjK4f7uVJlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.rand(in_features, out_feature, requires_grad=True) # 重置資料"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyKkDW0mdM86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "Vx = Variable(x, requires_grad = False)\n",
        "Vy = Variable(y, requires_grad = False)\n",
        "Vw = Variable(w, requires_grad = True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfLs2Q24aUK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f876c055-fae4-4ae6-81d4-ca5424da8457"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_predict = Vx.mm(Vw) # y-hat\n",
        "  loss = (y_predict - Vy).pow(2).mean()\n",
        "  print(\"Epoch: {}, Loss: {:.4f}\".format(epoch,loss))\n",
        "  loss.backward() # 在所有和loss有關Tensor，會自動做運算\n",
        "  Vw.data -= leaning_rate * Vw.grad.data # 因Call by reference所以同 w-=leaning_rate*Vw.grad.data\n",
        "  Vw.grad.data.zero_()\n",
        "Vw.data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 20.3573\n",
            "Epoch: 1, Loss: 19.6859\n",
            "Epoch: 2, Loss: 19.0375\n",
            "Epoch: 3, Loss: 18.4113\n",
            "Epoch: 4, Loss: 17.8065\n",
            "Epoch: 5, Loss: 17.2223\n",
            "Epoch: 6, Loss: 16.6582\n",
            "Epoch: 7, Loss: 16.1134\n",
            "Epoch: 8, Loss: 15.5872\n",
            "Epoch: 9, Loss: 15.0790\n",
            "Epoch: 10, Loss: 14.5881\n",
            "Epoch: 11, Loss: 14.1141\n",
            "Epoch: 12, Loss: 13.6562\n",
            "Epoch: 13, Loss: 13.2141\n",
            "Epoch: 14, Loss: 12.7870\n",
            "Epoch: 15, Loss: 12.3745\n",
            "Epoch: 16, Loss: 11.9761\n",
            "Epoch: 17, Loss: 11.5913\n",
            "Epoch: 18, Loss: 11.2197\n",
            "Epoch: 19, Loss: 10.8608\n",
            "Epoch: 20, Loss: 10.5141\n",
            "Epoch: 21, Loss: 10.1792\n",
            "Epoch: 22, Loss: 9.8558\n",
            "Epoch: 23, Loss: 9.5435\n",
            "Epoch: 24, Loss: 9.2418\n",
            "Epoch: 25, Loss: 8.9503\n",
            "Epoch: 26, Loss: 8.6689\n",
            "Epoch: 27, Loss: 8.3970\n",
            "Epoch: 28, Loss: 8.1344\n",
            "Epoch: 29, Loss: 7.8807\n",
            "Epoch: 30, Loss: 7.6357\n",
            "Epoch: 31, Loss: 7.3991\n",
            "Epoch: 32, Loss: 7.1705\n",
            "Epoch: 33, Loss: 6.9497\n",
            "Epoch: 34, Loss: 6.7364\n",
            "Epoch: 35, Loss: 6.5304\n",
            "Epoch: 36, Loss: 6.3314\n",
            "Epoch: 37, Loss: 6.1392\n",
            "Epoch: 38, Loss: 5.9535\n",
            "Epoch: 39, Loss: 5.7741\n",
            "Epoch: 40, Loss: 5.6008\n",
            "Epoch: 41, Loss: 5.4334\n",
            "Epoch: 42, Loss: 5.2717\n",
            "Epoch: 43, Loss: 5.1155\n",
            "Epoch: 44, Loss: 4.9646\n",
            "Epoch: 45, Loss: 4.8188\n",
            "Epoch: 46, Loss: 4.6780\n",
            "Epoch: 47, Loss: 4.5420\n",
            "Epoch: 48, Loss: 4.4105\n",
            "Epoch: 49, Loss: 4.2836\n",
            "Epoch: 50, Loss: 4.1609\n",
            "Epoch: 51, Loss: 4.0423\n",
            "Epoch: 52, Loss: 3.9278\n",
            "Epoch: 53, Loss: 3.8172\n",
            "Epoch: 54, Loss: 3.7103\n",
            "Epoch: 55, Loss: 3.6070\n",
            "Epoch: 56, Loss: 3.5073\n",
            "Epoch: 57, Loss: 3.4108\n",
            "Epoch: 58, Loss: 3.3177\n",
            "Epoch: 59, Loss: 3.2277\n",
            "Epoch: 60, Loss: 3.1407\n",
            "Epoch: 61, Loss: 3.0566\n",
            "Epoch: 62, Loss: 2.9754\n",
            "Epoch: 63, Loss: 2.8969\n",
            "Epoch: 64, Loss: 2.8211\n",
            "Epoch: 65, Loss: 2.7478\n",
            "Epoch: 66, Loss: 2.6770\n",
            "Epoch: 67, Loss: 2.6085\n",
            "Epoch: 68, Loss: 2.5424\n",
            "Epoch: 69, Loss: 2.4785\n",
            "Epoch: 70, Loss: 2.4167\n",
            "Epoch: 71, Loss: 2.3569\n",
            "Epoch: 72, Loss: 2.2992\n",
            "Epoch: 73, Loss: 2.2434\n",
            "Epoch: 74, Loss: 2.1895\n",
            "Epoch: 75, Loss: 2.1374\n",
            "Epoch: 76, Loss: 2.0870\n",
            "Epoch: 77, Loss: 2.0383\n",
            "Epoch: 78, Loss: 1.9912\n",
            "Epoch: 79, Loss: 1.9457\n",
            "Epoch: 80, Loss: 1.9016\n",
            "Epoch: 81, Loss: 1.8591\n",
            "Epoch: 82, Loss: 1.8179\n",
            "Epoch: 83, Loss: 1.7782\n",
            "Epoch: 84, Loss: 1.7397\n",
            "Epoch: 85, Loss: 1.7025\n",
            "Epoch: 86, Loss: 1.6665\n",
            "Epoch: 87, Loss: 1.6318\n",
            "Epoch: 88, Loss: 1.5981\n",
            "Epoch: 89, Loss: 1.5656\n",
            "Epoch: 90, Loss: 1.5341\n",
            "Epoch: 91, Loss: 1.5037\n",
            "Epoch: 92, Loss: 1.4743\n",
            "Epoch: 93, Loss: 1.4458\n",
            "Epoch: 94, Loss: 1.4182\n",
            "Epoch: 95, Loss: 1.3916\n",
            "Epoch: 96, Loss: 1.3658\n",
            "Epoch: 97, Loss: 1.3409\n",
            "Epoch: 98, Loss: 1.3167\n",
            "Epoch: 99, Loss: 1.2934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.8193],\n",
              "        [2.3702],\n",
              "        [2.8678]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-pvAKglgO_K",
        "colab_type": "text"
      },
      "source": [
        "## nn.Module "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnZ1DJ4vgg1B",
        "colab_type": "text"
      },
      "source": [
        "可自訂特殊Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA2EHZ4iegaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Z1ZPuagsTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 繼承 nn.Module\n",
        "class Model(nn.Module):\n",
        "  # Constructor 建構子\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__() # 呼叫父類別的建構子\n",
        "\n",
        "  def forward(self, x, w):\n",
        "    y_pred = x.mm(w)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "  # backward 大多不寫\n",
        "  def backward(self):\n",
        "    pass"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZsXewKviPsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "Vw = Variable(torch.rand(in_features, out_feature), requires_grad = True) # 會影響 param.data"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rn0xwjivUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ec229e5-316b-4178-fc70-434e0f25978b"
      },
      "source": [
        "for epoch in range(epoch_n):\n",
        "  y_predict = model(Vx,Vw)\n",
        "  loss = (y_predict - Vy).pow(2).mean()\n",
        "  print(\"Epoch: {}, Loss: {:.4f}\".format(epoch,loss))\n",
        "  loss.backward() # 在所有和loss有關Tensor，會自動做運算\n",
        "  Vw.data -= leaning_rate * Vw.grad.data # 因Call by reference所以同 w-=leaning_rate*Vw.grad.data\n",
        "  Vw.grad.data.zero_()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 20.1709\n",
            "Epoch: 1, Loss: 19.4981\n",
            "Epoch: 2, Loss: 18.8484\n",
            "Epoch: 3, Loss: 18.2208\n",
            "Epoch: 4, Loss: 17.6148\n",
            "Epoch: 5, Loss: 17.0296\n",
            "Epoch: 6, Loss: 16.4643\n",
            "Epoch: 7, Loss: 15.9185\n",
            "Epoch: 8, Loss: 15.3913\n",
            "Epoch: 9, Loss: 14.8822\n",
            "Epoch: 10, Loss: 14.3905\n",
            "Epoch: 11, Loss: 13.9156\n",
            "Epoch: 12, Loss: 13.4570\n",
            "Epoch: 13, Loss: 13.0141\n",
            "Epoch: 14, Loss: 12.5863\n",
            "Epoch: 15, Loss: 12.1732\n",
            "Epoch: 16, Loss: 11.7742\n",
            "Epoch: 17, Loss: 11.3889\n",
            "Epoch: 18, Loss: 11.0168\n",
            "Epoch: 19, Loss: 10.6573\n",
            "Epoch: 20, Loss: 10.3102\n",
            "Epoch: 21, Loss: 9.9750\n",
            "Epoch: 22, Loss: 9.6512\n",
            "Epoch: 23, Loss: 9.3385\n",
            "Epoch: 24, Loss: 9.0364\n",
            "Epoch: 25, Loss: 8.7448\n",
            "Epoch: 26, Loss: 8.4630\n",
            "Epoch: 27, Loss: 8.1909\n",
            "Epoch: 28, Loss: 7.9281\n",
            "Epoch: 29, Loss: 7.6743\n",
            "Epoch: 30, Loss: 7.4292\n",
            "Epoch: 31, Loss: 7.1924\n",
            "Epoch: 32, Loss: 6.9637\n",
            "Epoch: 33, Loss: 6.7428\n",
            "Epoch: 34, Loss: 6.5295\n",
            "Epoch: 35, Loss: 6.3234\n",
            "Epoch: 36, Loss: 6.1244\n",
            "Epoch: 37, Loss: 5.9322\n",
            "Epoch: 38, Loss: 5.7465\n",
            "Epoch: 39, Loss: 5.5672\n",
            "Epoch: 40, Loss: 5.3940\n",
            "Epoch: 41, Loss: 5.2267\n",
            "Epoch: 42, Loss: 5.0651\n",
            "Epoch: 43, Loss: 4.9090\n",
            "Epoch: 44, Loss: 4.7582\n",
            "Epoch: 45, Loss: 4.6125\n",
            "Epoch: 46, Loss: 4.4719\n",
            "Epoch: 47, Loss: 4.3360\n",
            "Epoch: 48, Loss: 4.2047\n",
            "Epoch: 49, Loss: 4.0779\n",
            "Epoch: 50, Loss: 3.9554\n",
            "Epoch: 51, Loss: 3.8371\n",
            "Epoch: 52, Loss: 3.7228\n",
            "Epoch: 53, Loss: 3.6124\n",
            "Epoch: 54, Loss: 3.5058\n",
            "Epoch: 55, Loss: 3.4028\n",
            "Epoch: 56, Loss: 3.3032\n",
            "Epoch: 57, Loss: 3.2071\n",
            "Epoch: 58, Loss: 3.1142\n",
            "Epoch: 59, Loss: 3.0245\n",
            "Epoch: 60, Loss: 2.9378\n",
            "Epoch: 61, Loss: 2.8541\n",
            "Epoch: 62, Loss: 2.7732\n",
            "Epoch: 63, Loss: 2.6950\n",
            "Epoch: 64, Loss: 2.6195\n",
            "Epoch: 65, Loss: 2.5465\n",
            "Epoch: 66, Loss: 2.4760\n",
            "Epoch: 67, Loss: 2.4079\n",
            "Epoch: 68, Loss: 2.3421\n",
            "Epoch: 69, Loss: 2.2785\n",
            "Epoch: 70, Loss: 2.2171\n",
            "Epoch: 71, Loss: 2.1577\n",
            "Epoch: 72, Loss: 2.1004\n",
            "Epoch: 73, Loss: 2.0450\n",
            "Epoch: 74, Loss: 1.9914\n",
            "Epoch: 75, Loss: 1.9397\n",
            "Epoch: 76, Loss: 1.8897\n",
            "Epoch: 77, Loss: 1.8413\n",
            "Epoch: 78, Loss: 1.7946\n",
            "Epoch: 79, Loss: 1.7495\n",
            "Epoch: 80, Loss: 1.7059\n",
            "Epoch: 81, Loss: 1.6637\n",
            "Epoch: 82, Loss: 1.6230\n",
            "Epoch: 83, Loss: 1.5836\n",
            "Epoch: 84, Loss: 1.5456\n",
            "Epoch: 85, Loss: 1.5088\n",
            "Epoch: 86, Loss: 1.4732\n",
            "Epoch: 87, Loss: 1.4389\n",
            "Epoch: 88, Loss: 1.4057\n",
            "Epoch: 89, Loss: 1.3735\n",
            "Epoch: 90, Loss: 1.3425\n",
            "Epoch: 91, Loss: 1.3125\n",
            "Epoch: 92, Loss: 1.2835\n",
            "Epoch: 93, Loss: 1.2554\n",
            "Epoch: 94, Loss: 1.2283\n",
            "Epoch: 95, Loss: 1.2021\n",
            "Epoch: 96, Loss: 1.1768\n",
            "Epoch: 97, Loss: 1.1522\n",
            "Epoch: 98, Loss: 1.1285\n",
            "Epoch: 99, Loss: 1.1056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJEIyVD6j1Lp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e2ca2df1-931d-4f3c-dc87-0ac75c3763c4"
      },
      "source": [
        "Vw.data"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.2135],\n",
              "        [2.6368],\n",
              "        [2.2506]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBmeTP_vj4ZA",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fenBmIyhkFgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__() # 呼叫父類別的建構子\n",
        "    self.linear = nn.Linear(in_features, out_feature, False) # 沒有常數項 False\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "  # backward 大多不寫\n",
        "  def backward(self):\n",
        "    pass"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYtfTknhlHvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "77a95527-8b12-480d-e591-f848d804696e"
      },
      "source": [
        "model = Model()\n",
        "loss_func = nn.MSELoss()\n",
        "print(model)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIr2uq54lg35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12736b10-a248-4fb7-ab0d-704ce9e65443"
      },
      "source": [
        "for epoch in range(epoch_n * 10): # epoch 10倍\n",
        "  y_predict = model(Vx)\n",
        "  loss = loss_func(y_predict, y) # y_predict,y的順序要注意\n",
        "  print(\"Epoch: {}, Loss: {:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad() # 會產生隨機 Entropy ，所以要搶先清除?\n",
        "  loss.backward() # 在所有和loss有關Tensor，會自動做運算\n",
        "  for param in model.parameters(): # 從各個 Layer 取\n",
        "    param.data -= leaning_rate * param.grad.data"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 25.1659\n",
            "Epoch: 1, Loss: 24.3216\n",
            "Epoch: 2, Loss: 23.5062\n",
            "Epoch: 3, Loss: 22.7187\n",
            "Epoch: 4, Loss: 21.9583\n",
            "Epoch: 5, Loss: 21.2239\n",
            "Epoch: 6, Loss: 20.5146\n",
            "Epoch: 7, Loss: 19.8296\n",
            "Epoch: 8, Loss: 19.1681\n",
            "Epoch: 9, Loss: 18.5293\n",
            "Epoch: 10, Loss: 17.9124\n",
            "Epoch: 11, Loss: 17.3165\n",
            "Epoch: 12, Loss: 16.7411\n",
            "Epoch: 13, Loss: 16.1854\n",
            "Epoch: 14, Loss: 15.6487\n",
            "Epoch: 15, Loss: 15.1304\n",
            "Epoch: 16, Loss: 14.6298\n",
            "Epoch: 17, Loss: 14.1464\n",
            "Epoch: 18, Loss: 13.6795\n",
            "Epoch: 19, Loss: 13.2286\n",
            "Epoch: 20, Loss: 12.7932\n",
            "Epoch: 21, Loss: 12.3726\n",
            "Epoch: 22, Loss: 11.9664\n",
            "Epoch: 23, Loss: 11.5742\n",
            "Epoch: 24, Loss: 11.1953\n",
            "Epoch: 25, Loss: 10.8294\n",
            "Epoch: 26, Loss: 10.4761\n",
            "Epoch: 27, Loss: 10.1348\n",
            "Epoch: 28, Loss: 9.8052\n",
            "Epoch: 29, Loss: 9.4868\n",
            "Epoch: 30, Loss: 9.1794\n",
            "Epoch: 31, Loss: 8.8824\n",
            "Epoch: 32, Loss: 8.5956\n",
            "Epoch: 33, Loss: 8.3187\n",
            "Epoch: 34, Loss: 8.0511\n",
            "Epoch: 35, Loss: 7.7928\n",
            "Epoch: 36, Loss: 7.5432\n",
            "Epoch: 37, Loss: 7.3022\n",
            "Epoch: 38, Loss: 7.0694\n",
            "Epoch: 39, Loss: 6.8446\n",
            "Epoch: 40, Loss: 6.6274\n",
            "Epoch: 41, Loss: 6.4177\n",
            "Epoch: 42, Loss: 6.2151\n",
            "Epoch: 43, Loss: 6.0194\n",
            "Epoch: 44, Loss: 5.8304\n",
            "Epoch: 45, Loss: 5.6479\n",
            "Epoch: 46, Loss: 5.4716\n",
            "Epoch: 47, Loss: 5.3013\n",
            "Epoch: 48, Loss: 5.1368\n",
            "Epoch: 49, Loss: 4.9779\n",
            "Epoch: 50, Loss: 4.8244\n",
            "Epoch: 51, Loss: 4.6762\n",
            "Epoch: 52, Loss: 4.5330\n",
            "Epoch: 53, Loss: 4.3947\n",
            "Epoch: 54, Loss: 4.2611\n",
            "Epoch: 55, Loss: 4.1320\n",
            "Epoch: 56, Loss: 4.0074\n",
            "Epoch: 57, Loss: 3.8869\n",
            "Epoch: 58, Loss: 3.7706\n",
            "Epoch: 59, Loss: 3.6583\n",
            "Epoch: 60, Loss: 3.5497\n",
            "Epoch: 61, Loss: 3.4449\n",
            "Epoch: 62, Loss: 3.3436\n",
            "Epoch: 63, Loss: 3.2457\n",
            "Epoch: 64, Loss: 3.1512\n",
            "Epoch: 65, Loss: 3.0599\n",
            "Epoch: 66, Loss: 2.9717\n",
            "Epoch: 67, Loss: 2.8865\n",
            "Epoch: 68, Loss: 2.8041\n",
            "Epoch: 69, Loss: 2.7246\n",
            "Epoch: 70, Loss: 2.6477\n",
            "Epoch: 71, Loss: 2.5735\n",
            "Epoch: 72, Loss: 2.5018\n",
            "Epoch: 73, Loss: 2.4325\n",
            "Epoch: 74, Loss: 2.3655\n",
            "Epoch: 75, Loss: 2.3008\n",
            "Epoch: 76, Loss: 2.2383\n",
            "Epoch: 77, Loss: 2.1779\n",
            "Epoch: 78, Loss: 2.1196\n",
            "Epoch: 79, Loss: 2.0632\n",
            "Epoch: 80, Loss: 2.0087\n",
            "Epoch: 81, Loss: 1.9560\n",
            "Epoch: 82, Loss: 1.9052\n",
            "Epoch: 83, Loss: 1.8560\n",
            "Epoch: 84, Loss: 1.8085\n",
            "Epoch: 85, Loss: 1.7626\n",
            "Epoch: 86, Loss: 1.7182\n",
            "Epoch: 87, Loss: 1.6753\n",
            "Epoch: 88, Loss: 1.6339\n",
            "Epoch: 89, Loss: 1.5938\n",
            "Epoch: 90, Loss: 1.5551\n",
            "Epoch: 91, Loss: 1.5177\n",
            "Epoch: 92, Loss: 1.4816\n",
            "Epoch: 93, Loss: 1.4466\n",
            "Epoch: 94, Loss: 1.4128\n",
            "Epoch: 95, Loss: 1.3802\n",
            "Epoch: 96, Loss: 1.3486\n",
            "Epoch: 97, Loss: 1.3181\n",
            "Epoch: 98, Loss: 1.2886\n",
            "Epoch: 99, Loss: 1.2601\n",
            "Epoch: 100, Loss: 1.2325\n",
            "Epoch: 101, Loss: 1.2058\n",
            "Epoch: 102, Loss: 1.1801\n",
            "Epoch: 103, Loss: 1.1552\n",
            "Epoch: 104, Loss: 1.1311\n",
            "Epoch: 105, Loss: 1.1078\n",
            "Epoch: 106, Loss: 1.0852\n",
            "Epoch: 107, Loss: 1.0634\n",
            "Epoch: 108, Loss: 1.0424\n",
            "Epoch: 109, Loss: 1.0220\n",
            "Epoch: 110, Loss: 1.0023\n",
            "Epoch: 111, Loss: 0.9832\n",
            "Epoch: 112, Loss: 0.9648\n",
            "Epoch: 113, Loss: 0.9469\n",
            "Epoch: 114, Loss: 0.9297\n",
            "Epoch: 115, Loss: 0.9130\n",
            "Epoch: 116, Loss: 0.8968\n",
            "Epoch: 117, Loss: 0.8812\n",
            "Epoch: 118, Loss: 0.8661\n",
            "Epoch: 119, Loss: 0.8514\n",
            "Epoch: 120, Loss: 0.8373\n",
            "Epoch: 121, Loss: 0.8236\n",
            "Epoch: 122, Loss: 0.8103\n",
            "Epoch: 123, Loss: 0.7975\n",
            "Epoch: 124, Loss: 0.7850\n",
            "Epoch: 125, Loss: 0.7730\n",
            "Epoch: 126, Loss: 0.7614\n",
            "Epoch: 127, Loss: 0.7501\n",
            "Epoch: 128, Loss: 0.7392\n",
            "Epoch: 129, Loss: 0.7286\n",
            "Epoch: 130, Loss: 0.7184\n",
            "Epoch: 131, Loss: 0.7084\n",
            "Epoch: 132, Loss: 0.6988\n",
            "Epoch: 133, Loss: 0.6895\n",
            "Epoch: 134, Loss: 0.6805\n",
            "Epoch: 135, Loss: 0.6718\n",
            "Epoch: 136, Loss: 0.6633\n",
            "Epoch: 137, Loss: 0.6551\n",
            "Epoch: 138, Loss: 0.6472\n",
            "Epoch: 139, Loss: 0.6395\n",
            "Epoch: 140, Loss: 0.6320\n",
            "Epoch: 141, Loss: 0.6248\n",
            "Epoch: 142, Loss: 0.6177\n",
            "Epoch: 143, Loss: 0.6109\n",
            "Epoch: 144, Loss: 0.6043\n",
            "Epoch: 145, Loss: 0.5979\n",
            "Epoch: 146, Loss: 0.5917\n",
            "Epoch: 147, Loss: 0.5857\n",
            "Epoch: 148, Loss: 0.5798\n",
            "Epoch: 149, Loss: 0.5742\n",
            "Epoch: 150, Loss: 0.5687\n",
            "Epoch: 151, Loss: 0.5633\n",
            "Epoch: 152, Loss: 0.5581\n",
            "Epoch: 153, Loss: 0.5531\n",
            "Epoch: 154, Loss: 0.5482\n",
            "Epoch: 155, Loss: 0.5435\n",
            "Epoch: 156, Loss: 0.5388\n",
            "Epoch: 157, Loss: 0.5343\n",
            "Epoch: 158, Loss: 0.5300\n",
            "Epoch: 159, Loss: 0.5258\n",
            "Epoch: 160, Loss: 0.5216\n",
            "Epoch: 161, Loss: 0.5176\n",
            "Epoch: 162, Loss: 0.5137\n",
            "Epoch: 163, Loss: 0.5100\n",
            "Epoch: 164, Loss: 0.5063\n",
            "Epoch: 165, Loss: 0.5027\n",
            "Epoch: 166, Loss: 0.4992\n",
            "Epoch: 167, Loss: 0.4958\n",
            "Epoch: 168, Loss: 0.4925\n",
            "Epoch: 169, Loss: 0.4893\n",
            "Epoch: 170, Loss: 0.4862\n",
            "Epoch: 171, Loss: 0.4831\n",
            "Epoch: 172, Loss: 0.4801\n",
            "Epoch: 173, Loss: 0.4773\n",
            "Epoch: 174, Loss: 0.4744\n",
            "Epoch: 175, Loss: 0.4717\n",
            "Epoch: 176, Loss: 0.4690\n",
            "Epoch: 177, Loss: 0.4664\n",
            "Epoch: 178, Loss: 0.4638\n",
            "Epoch: 179, Loss: 0.4613\n",
            "Epoch: 180, Loss: 0.4589\n",
            "Epoch: 181, Loss: 0.4565\n",
            "Epoch: 182, Loss: 0.4542\n",
            "Epoch: 183, Loss: 0.4520\n",
            "Epoch: 184, Loss: 0.4497\n",
            "Epoch: 185, Loss: 0.4476\n",
            "Epoch: 186, Loss: 0.4455\n",
            "Epoch: 187, Loss: 0.4434\n",
            "Epoch: 188, Loss: 0.4414\n",
            "Epoch: 189, Loss: 0.4394\n",
            "Epoch: 190, Loss: 0.4375\n",
            "Epoch: 191, Loss: 0.4356\n",
            "Epoch: 192, Loss: 0.4338\n",
            "Epoch: 193, Loss: 0.4319\n",
            "Epoch: 194, Loss: 0.4302\n",
            "Epoch: 195, Loss: 0.4284\n",
            "Epoch: 196, Loss: 0.4267\n",
            "Epoch: 197, Loss: 0.4251\n",
            "Epoch: 198, Loss: 0.4234\n",
            "Epoch: 199, Loss: 0.4218\n",
            "Epoch: 200, Loss: 0.4203\n",
            "Epoch: 201, Loss: 0.4187\n",
            "Epoch: 202, Loss: 0.4172\n",
            "Epoch: 203, Loss: 0.4157\n",
            "Epoch: 204, Loss: 0.4143\n",
            "Epoch: 205, Loss: 0.4128\n",
            "Epoch: 206, Loss: 0.4114\n",
            "Epoch: 207, Loss: 0.4100\n",
            "Epoch: 208, Loss: 0.4087\n",
            "Epoch: 209, Loss: 0.4073\n",
            "Epoch: 210, Loss: 0.4060\n",
            "Epoch: 211, Loss: 0.4047\n",
            "Epoch: 212, Loss: 0.4035\n",
            "Epoch: 213, Loss: 0.4022\n",
            "Epoch: 214, Loss: 0.4010\n",
            "Epoch: 215, Loss: 0.3998\n",
            "Epoch: 216, Loss: 0.3986\n",
            "Epoch: 217, Loss: 0.3974\n",
            "Epoch: 218, Loss: 0.3962\n",
            "Epoch: 219, Loss: 0.3951\n",
            "Epoch: 220, Loss: 0.3940\n",
            "Epoch: 221, Loss: 0.3928\n",
            "Epoch: 222, Loss: 0.3917\n",
            "Epoch: 223, Loss: 0.3907\n",
            "Epoch: 224, Loss: 0.3896\n",
            "Epoch: 225, Loss: 0.3885\n",
            "Epoch: 226, Loss: 0.3875\n",
            "Epoch: 227, Loss: 0.3865\n",
            "Epoch: 228, Loss: 0.3855\n",
            "Epoch: 229, Loss: 0.3845\n",
            "Epoch: 230, Loss: 0.3835\n",
            "Epoch: 231, Loss: 0.3825\n",
            "Epoch: 232, Loss: 0.3815\n",
            "Epoch: 233, Loss: 0.3806\n",
            "Epoch: 234, Loss: 0.3796\n",
            "Epoch: 235, Loss: 0.3787\n",
            "Epoch: 236, Loss: 0.3778\n",
            "Epoch: 237, Loss: 0.3768\n",
            "Epoch: 238, Loss: 0.3759\n",
            "Epoch: 239, Loss: 0.3750\n",
            "Epoch: 240, Loss: 0.3742\n",
            "Epoch: 241, Loss: 0.3733\n",
            "Epoch: 242, Loss: 0.3724\n",
            "Epoch: 243, Loss: 0.3715\n",
            "Epoch: 244, Loss: 0.3707\n",
            "Epoch: 245, Loss: 0.3698\n",
            "Epoch: 246, Loss: 0.3690\n",
            "Epoch: 247, Loss: 0.3682\n",
            "Epoch: 248, Loss: 0.3673\n",
            "Epoch: 249, Loss: 0.3665\n",
            "Epoch: 250, Loss: 0.3657\n",
            "Epoch: 251, Loss: 0.3649\n",
            "Epoch: 252, Loss: 0.3641\n",
            "Epoch: 253, Loss: 0.3633\n",
            "Epoch: 254, Loss: 0.3625\n",
            "Epoch: 255, Loss: 0.3617\n",
            "Epoch: 256, Loss: 0.3610\n",
            "Epoch: 257, Loss: 0.3602\n",
            "Epoch: 258, Loss: 0.3594\n",
            "Epoch: 259, Loss: 0.3587\n",
            "Epoch: 260, Loss: 0.3579\n",
            "Epoch: 261, Loss: 0.3572\n",
            "Epoch: 262, Loss: 0.3564\n",
            "Epoch: 263, Loss: 0.3557\n",
            "Epoch: 264, Loss: 0.3549\n",
            "Epoch: 265, Loss: 0.3542\n",
            "Epoch: 266, Loss: 0.3535\n",
            "Epoch: 267, Loss: 0.3528\n",
            "Epoch: 268, Loss: 0.3521\n",
            "Epoch: 269, Loss: 0.3513\n",
            "Epoch: 270, Loss: 0.3506\n",
            "Epoch: 271, Loss: 0.3499\n",
            "Epoch: 272, Loss: 0.3492\n",
            "Epoch: 273, Loss: 0.3485\n",
            "Epoch: 274, Loss: 0.3478\n",
            "Epoch: 275, Loss: 0.3472\n",
            "Epoch: 276, Loss: 0.3465\n",
            "Epoch: 277, Loss: 0.3458\n",
            "Epoch: 278, Loss: 0.3451\n",
            "Epoch: 279, Loss: 0.3444\n",
            "Epoch: 280, Loss: 0.3438\n",
            "Epoch: 281, Loss: 0.3431\n",
            "Epoch: 282, Loss: 0.3424\n",
            "Epoch: 283, Loss: 0.3418\n",
            "Epoch: 284, Loss: 0.3411\n",
            "Epoch: 285, Loss: 0.3405\n",
            "Epoch: 286, Loss: 0.3398\n",
            "Epoch: 287, Loss: 0.3392\n",
            "Epoch: 288, Loss: 0.3385\n",
            "Epoch: 289, Loss: 0.3379\n",
            "Epoch: 290, Loss: 0.3372\n",
            "Epoch: 291, Loss: 0.3366\n",
            "Epoch: 292, Loss: 0.3360\n",
            "Epoch: 293, Loss: 0.3353\n",
            "Epoch: 294, Loss: 0.3347\n",
            "Epoch: 295, Loss: 0.3341\n",
            "Epoch: 296, Loss: 0.3334\n",
            "Epoch: 297, Loss: 0.3328\n",
            "Epoch: 298, Loss: 0.3322\n",
            "Epoch: 299, Loss: 0.3316\n",
            "Epoch: 300, Loss: 0.3310\n",
            "Epoch: 301, Loss: 0.3303\n",
            "Epoch: 302, Loss: 0.3297\n",
            "Epoch: 303, Loss: 0.3291\n",
            "Epoch: 304, Loss: 0.3285\n",
            "Epoch: 305, Loss: 0.3279\n",
            "Epoch: 306, Loss: 0.3273\n",
            "Epoch: 307, Loss: 0.3267\n",
            "Epoch: 308, Loss: 0.3261\n",
            "Epoch: 309, Loss: 0.3255\n",
            "Epoch: 310, Loss: 0.3249\n",
            "Epoch: 311, Loss: 0.3243\n",
            "Epoch: 312, Loss: 0.3238\n",
            "Epoch: 313, Loss: 0.3232\n",
            "Epoch: 314, Loss: 0.3226\n",
            "Epoch: 315, Loss: 0.3220\n",
            "Epoch: 316, Loss: 0.3214\n",
            "Epoch: 317, Loss: 0.3208\n",
            "Epoch: 318, Loss: 0.3203\n",
            "Epoch: 319, Loss: 0.3197\n",
            "Epoch: 320, Loss: 0.3191\n",
            "Epoch: 321, Loss: 0.3185\n",
            "Epoch: 322, Loss: 0.3180\n",
            "Epoch: 323, Loss: 0.3174\n",
            "Epoch: 324, Loss: 0.3168\n",
            "Epoch: 325, Loss: 0.3163\n",
            "Epoch: 326, Loss: 0.3157\n",
            "Epoch: 327, Loss: 0.3151\n",
            "Epoch: 328, Loss: 0.3146\n",
            "Epoch: 329, Loss: 0.3140\n",
            "Epoch: 330, Loss: 0.3135\n",
            "Epoch: 331, Loss: 0.3129\n",
            "Epoch: 332, Loss: 0.3124\n",
            "Epoch: 333, Loss: 0.3118\n",
            "Epoch: 334, Loss: 0.3113\n",
            "Epoch: 335, Loss: 0.3107\n",
            "Epoch: 336, Loss: 0.3102\n",
            "Epoch: 337, Loss: 0.3096\n",
            "Epoch: 338, Loss: 0.3091\n",
            "Epoch: 339, Loss: 0.3086\n",
            "Epoch: 340, Loss: 0.3080\n",
            "Epoch: 341, Loss: 0.3075\n",
            "Epoch: 342, Loss: 0.3069\n",
            "Epoch: 343, Loss: 0.3064\n",
            "Epoch: 344, Loss: 0.3059\n",
            "Epoch: 345, Loss: 0.3053\n",
            "Epoch: 346, Loss: 0.3048\n",
            "Epoch: 347, Loss: 0.3043\n",
            "Epoch: 348, Loss: 0.3038\n",
            "Epoch: 349, Loss: 0.3032\n",
            "Epoch: 350, Loss: 0.3027\n",
            "Epoch: 351, Loss: 0.3022\n",
            "Epoch: 352, Loss: 0.3017\n",
            "Epoch: 353, Loss: 0.3011\n",
            "Epoch: 354, Loss: 0.3006\n",
            "Epoch: 355, Loss: 0.3001\n",
            "Epoch: 356, Loss: 0.2996\n",
            "Epoch: 357, Loss: 0.2991\n",
            "Epoch: 358, Loss: 0.2986\n",
            "Epoch: 359, Loss: 0.2981\n",
            "Epoch: 360, Loss: 0.2975\n",
            "Epoch: 361, Loss: 0.2970\n",
            "Epoch: 362, Loss: 0.2965\n",
            "Epoch: 363, Loss: 0.2960\n",
            "Epoch: 364, Loss: 0.2955\n",
            "Epoch: 365, Loss: 0.2950\n",
            "Epoch: 366, Loss: 0.2945\n",
            "Epoch: 367, Loss: 0.2940\n",
            "Epoch: 368, Loss: 0.2935\n",
            "Epoch: 369, Loss: 0.2930\n",
            "Epoch: 370, Loss: 0.2925\n",
            "Epoch: 371, Loss: 0.2920\n",
            "Epoch: 372, Loss: 0.2915\n",
            "Epoch: 373, Loss: 0.2910\n",
            "Epoch: 374, Loss: 0.2905\n",
            "Epoch: 375, Loss: 0.2901\n",
            "Epoch: 376, Loss: 0.2896\n",
            "Epoch: 377, Loss: 0.2891\n",
            "Epoch: 378, Loss: 0.2886\n",
            "Epoch: 379, Loss: 0.2881\n",
            "Epoch: 380, Loss: 0.2876\n",
            "Epoch: 381, Loss: 0.2872\n",
            "Epoch: 382, Loss: 0.2867\n",
            "Epoch: 383, Loss: 0.2862\n",
            "Epoch: 384, Loss: 0.2857\n",
            "Epoch: 385, Loss: 0.2852\n",
            "Epoch: 386, Loss: 0.2848\n",
            "Epoch: 387, Loss: 0.2843\n",
            "Epoch: 388, Loss: 0.2838\n",
            "Epoch: 389, Loss: 0.2833\n",
            "Epoch: 390, Loss: 0.2829\n",
            "Epoch: 391, Loss: 0.2824\n",
            "Epoch: 392, Loss: 0.2819\n",
            "Epoch: 393, Loss: 0.2815\n",
            "Epoch: 394, Loss: 0.2810\n",
            "Epoch: 395, Loss: 0.2805\n",
            "Epoch: 396, Loss: 0.2801\n",
            "Epoch: 397, Loss: 0.2796\n",
            "Epoch: 398, Loss: 0.2792\n",
            "Epoch: 399, Loss: 0.2787\n",
            "Epoch: 400, Loss: 0.2782\n",
            "Epoch: 401, Loss: 0.2778\n",
            "Epoch: 402, Loss: 0.2773\n",
            "Epoch: 403, Loss: 0.2769\n",
            "Epoch: 404, Loss: 0.2764\n",
            "Epoch: 405, Loss: 0.2760\n",
            "Epoch: 406, Loss: 0.2755\n",
            "Epoch: 407, Loss: 0.2751\n",
            "Epoch: 408, Loss: 0.2746\n",
            "Epoch: 409, Loss: 0.2742\n",
            "Epoch: 410, Loss: 0.2737\n",
            "Epoch: 411, Loss: 0.2733\n",
            "Epoch: 412, Loss: 0.2728\n",
            "Epoch: 413, Loss: 0.2724\n",
            "Epoch: 414, Loss: 0.2719\n",
            "Epoch: 415, Loss: 0.2715\n",
            "Epoch: 416, Loss: 0.2711\n",
            "Epoch: 417, Loss: 0.2706\n",
            "Epoch: 418, Loss: 0.2702\n",
            "Epoch: 419, Loss: 0.2698\n",
            "Epoch: 420, Loss: 0.2693\n",
            "Epoch: 421, Loss: 0.2689\n",
            "Epoch: 422, Loss: 0.2685\n",
            "Epoch: 423, Loss: 0.2680\n",
            "Epoch: 424, Loss: 0.2676\n",
            "Epoch: 425, Loss: 0.2672\n",
            "Epoch: 426, Loss: 0.2667\n",
            "Epoch: 427, Loss: 0.2663\n",
            "Epoch: 428, Loss: 0.2659\n",
            "Epoch: 429, Loss: 0.2655\n",
            "Epoch: 430, Loss: 0.2650\n",
            "Epoch: 431, Loss: 0.2646\n",
            "Epoch: 432, Loss: 0.2642\n",
            "Epoch: 433, Loss: 0.2638\n",
            "Epoch: 434, Loss: 0.2633\n",
            "Epoch: 435, Loss: 0.2629\n",
            "Epoch: 436, Loss: 0.2625\n",
            "Epoch: 437, Loss: 0.2621\n",
            "Epoch: 438, Loss: 0.2617\n",
            "Epoch: 439, Loss: 0.2613\n",
            "Epoch: 440, Loss: 0.2608\n",
            "Epoch: 441, Loss: 0.2604\n",
            "Epoch: 442, Loss: 0.2600\n",
            "Epoch: 443, Loss: 0.2596\n",
            "Epoch: 444, Loss: 0.2592\n",
            "Epoch: 445, Loss: 0.2588\n",
            "Epoch: 446, Loss: 0.2584\n",
            "Epoch: 447, Loss: 0.2580\n",
            "Epoch: 448, Loss: 0.2576\n",
            "Epoch: 449, Loss: 0.2572\n",
            "Epoch: 450, Loss: 0.2568\n",
            "Epoch: 451, Loss: 0.2564\n",
            "Epoch: 452, Loss: 0.2560\n",
            "Epoch: 453, Loss: 0.2556\n",
            "Epoch: 454, Loss: 0.2552\n",
            "Epoch: 455, Loss: 0.2548\n",
            "Epoch: 456, Loss: 0.2544\n",
            "Epoch: 457, Loss: 0.2540\n",
            "Epoch: 458, Loss: 0.2536\n",
            "Epoch: 459, Loss: 0.2532\n",
            "Epoch: 460, Loss: 0.2528\n",
            "Epoch: 461, Loss: 0.2524\n",
            "Epoch: 462, Loss: 0.2520\n",
            "Epoch: 463, Loss: 0.2516\n",
            "Epoch: 464, Loss: 0.2512\n",
            "Epoch: 465, Loss: 0.2509\n",
            "Epoch: 466, Loss: 0.2505\n",
            "Epoch: 467, Loss: 0.2501\n",
            "Epoch: 468, Loss: 0.2497\n",
            "Epoch: 469, Loss: 0.2493\n",
            "Epoch: 470, Loss: 0.2489\n",
            "Epoch: 471, Loss: 0.2485\n",
            "Epoch: 472, Loss: 0.2482\n",
            "Epoch: 473, Loss: 0.2478\n",
            "Epoch: 474, Loss: 0.2474\n",
            "Epoch: 475, Loss: 0.2470\n",
            "Epoch: 476, Loss: 0.2467\n",
            "Epoch: 477, Loss: 0.2463\n",
            "Epoch: 478, Loss: 0.2459\n",
            "Epoch: 479, Loss: 0.2455\n",
            "Epoch: 480, Loss: 0.2452\n",
            "Epoch: 481, Loss: 0.2448\n",
            "Epoch: 482, Loss: 0.2444\n",
            "Epoch: 483, Loss: 0.2440\n",
            "Epoch: 484, Loss: 0.2437\n",
            "Epoch: 485, Loss: 0.2433\n",
            "Epoch: 486, Loss: 0.2429\n",
            "Epoch: 487, Loss: 0.2426\n",
            "Epoch: 488, Loss: 0.2422\n",
            "Epoch: 489, Loss: 0.2418\n",
            "Epoch: 490, Loss: 0.2415\n",
            "Epoch: 491, Loss: 0.2411\n",
            "Epoch: 492, Loss: 0.2408\n",
            "Epoch: 493, Loss: 0.2404\n",
            "Epoch: 494, Loss: 0.2400\n",
            "Epoch: 495, Loss: 0.2397\n",
            "Epoch: 496, Loss: 0.2393\n",
            "Epoch: 497, Loss: 0.2390\n",
            "Epoch: 498, Loss: 0.2386\n",
            "Epoch: 499, Loss: 0.2382\n",
            "Epoch: 500, Loss: 0.2379\n",
            "Epoch: 501, Loss: 0.2375\n",
            "Epoch: 502, Loss: 0.2372\n",
            "Epoch: 503, Loss: 0.2368\n",
            "Epoch: 504, Loss: 0.2365\n",
            "Epoch: 505, Loss: 0.2361\n",
            "Epoch: 506, Loss: 0.2358\n",
            "Epoch: 507, Loss: 0.2354\n",
            "Epoch: 508, Loss: 0.2351\n",
            "Epoch: 509, Loss: 0.2347\n",
            "Epoch: 510, Loss: 0.2344\n",
            "Epoch: 511, Loss: 0.2341\n",
            "Epoch: 512, Loss: 0.2337\n",
            "Epoch: 513, Loss: 0.2334\n",
            "Epoch: 514, Loss: 0.2330\n",
            "Epoch: 515, Loss: 0.2327\n",
            "Epoch: 516, Loss: 0.2323\n",
            "Epoch: 517, Loss: 0.2320\n",
            "Epoch: 518, Loss: 0.2317\n",
            "Epoch: 519, Loss: 0.2313\n",
            "Epoch: 520, Loss: 0.2310\n",
            "Epoch: 521, Loss: 0.2307\n",
            "Epoch: 522, Loss: 0.2303\n",
            "Epoch: 523, Loss: 0.2300\n",
            "Epoch: 524, Loss: 0.2297\n",
            "Epoch: 525, Loss: 0.2293\n",
            "Epoch: 526, Loss: 0.2290\n",
            "Epoch: 527, Loss: 0.2287\n",
            "Epoch: 528, Loss: 0.2283\n",
            "Epoch: 529, Loss: 0.2280\n",
            "Epoch: 530, Loss: 0.2277\n",
            "Epoch: 531, Loss: 0.2273\n",
            "Epoch: 532, Loss: 0.2270\n",
            "Epoch: 533, Loss: 0.2267\n",
            "Epoch: 534, Loss: 0.2264\n",
            "Epoch: 535, Loss: 0.2260\n",
            "Epoch: 536, Loss: 0.2257\n",
            "Epoch: 537, Loss: 0.2254\n",
            "Epoch: 538, Loss: 0.2251\n",
            "Epoch: 539, Loss: 0.2248\n",
            "Epoch: 540, Loss: 0.2244\n",
            "Epoch: 541, Loss: 0.2241\n",
            "Epoch: 542, Loss: 0.2238\n",
            "Epoch: 543, Loss: 0.2235\n",
            "Epoch: 544, Loss: 0.2232\n",
            "Epoch: 545, Loss: 0.2228\n",
            "Epoch: 546, Loss: 0.2225\n",
            "Epoch: 547, Loss: 0.2222\n",
            "Epoch: 548, Loss: 0.2219\n",
            "Epoch: 549, Loss: 0.2216\n",
            "Epoch: 550, Loss: 0.2213\n",
            "Epoch: 551, Loss: 0.2210\n",
            "Epoch: 552, Loss: 0.2207\n",
            "Epoch: 553, Loss: 0.2203\n",
            "Epoch: 554, Loss: 0.2200\n",
            "Epoch: 555, Loss: 0.2197\n",
            "Epoch: 556, Loss: 0.2194\n",
            "Epoch: 557, Loss: 0.2191\n",
            "Epoch: 558, Loss: 0.2188\n",
            "Epoch: 559, Loss: 0.2185\n",
            "Epoch: 560, Loss: 0.2182\n",
            "Epoch: 561, Loss: 0.2179\n",
            "Epoch: 562, Loss: 0.2176\n",
            "Epoch: 563, Loss: 0.2173\n",
            "Epoch: 564, Loss: 0.2170\n",
            "Epoch: 565, Loss: 0.2167\n",
            "Epoch: 566, Loss: 0.2164\n",
            "Epoch: 567, Loss: 0.2161\n",
            "Epoch: 568, Loss: 0.2158\n",
            "Epoch: 569, Loss: 0.2155\n",
            "Epoch: 570, Loss: 0.2152\n",
            "Epoch: 571, Loss: 0.2149\n",
            "Epoch: 572, Loss: 0.2146\n",
            "Epoch: 573, Loss: 0.2143\n",
            "Epoch: 574, Loss: 0.2140\n",
            "Epoch: 575, Loss: 0.2137\n",
            "Epoch: 576, Loss: 0.2134\n",
            "Epoch: 577, Loss: 0.2132\n",
            "Epoch: 578, Loss: 0.2129\n",
            "Epoch: 579, Loss: 0.2126\n",
            "Epoch: 580, Loss: 0.2123\n",
            "Epoch: 581, Loss: 0.2120\n",
            "Epoch: 582, Loss: 0.2117\n",
            "Epoch: 583, Loss: 0.2114\n",
            "Epoch: 584, Loss: 0.2111\n",
            "Epoch: 585, Loss: 0.2109\n",
            "Epoch: 586, Loss: 0.2106\n",
            "Epoch: 587, Loss: 0.2103\n",
            "Epoch: 588, Loss: 0.2100\n",
            "Epoch: 589, Loss: 0.2097\n",
            "Epoch: 590, Loss: 0.2094\n",
            "Epoch: 591, Loss: 0.2092\n",
            "Epoch: 592, Loss: 0.2089\n",
            "Epoch: 593, Loss: 0.2086\n",
            "Epoch: 594, Loss: 0.2083\n",
            "Epoch: 595, Loss: 0.2080\n",
            "Epoch: 596, Loss: 0.2078\n",
            "Epoch: 597, Loss: 0.2075\n",
            "Epoch: 598, Loss: 0.2072\n",
            "Epoch: 599, Loss: 0.2069\n",
            "Epoch: 600, Loss: 0.2067\n",
            "Epoch: 601, Loss: 0.2064\n",
            "Epoch: 602, Loss: 0.2061\n",
            "Epoch: 603, Loss: 0.2058\n",
            "Epoch: 604, Loss: 0.2056\n",
            "Epoch: 605, Loss: 0.2053\n",
            "Epoch: 606, Loss: 0.2050\n",
            "Epoch: 607, Loss: 0.2048\n",
            "Epoch: 608, Loss: 0.2045\n",
            "Epoch: 609, Loss: 0.2042\n",
            "Epoch: 610, Loss: 0.2040\n",
            "Epoch: 611, Loss: 0.2037\n",
            "Epoch: 612, Loss: 0.2034\n",
            "Epoch: 613, Loss: 0.2032\n",
            "Epoch: 614, Loss: 0.2029\n",
            "Epoch: 615, Loss: 0.2026\n",
            "Epoch: 616, Loss: 0.2024\n",
            "Epoch: 617, Loss: 0.2021\n",
            "Epoch: 618, Loss: 0.2018\n",
            "Epoch: 619, Loss: 0.2016\n",
            "Epoch: 620, Loss: 0.2013\n",
            "Epoch: 621, Loss: 0.2011\n",
            "Epoch: 622, Loss: 0.2008\n",
            "Epoch: 623, Loss: 0.2005\n",
            "Epoch: 624, Loss: 0.2003\n",
            "Epoch: 625, Loss: 0.2000\n",
            "Epoch: 626, Loss: 0.1998\n",
            "Epoch: 627, Loss: 0.1995\n",
            "Epoch: 628, Loss: 0.1992\n",
            "Epoch: 629, Loss: 0.1990\n",
            "Epoch: 630, Loss: 0.1987\n",
            "Epoch: 631, Loss: 0.1985\n",
            "Epoch: 632, Loss: 0.1982\n",
            "Epoch: 633, Loss: 0.1980\n",
            "Epoch: 634, Loss: 0.1977\n",
            "Epoch: 635, Loss: 0.1975\n",
            "Epoch: 636, Loss: 0.1972\n",
            "Epoch: 637, Loss: 0.1970\n",
            "Epoch: 638, Loss: 0.1967\n",
            "Epoch: 639, Loss: 0.1965\n",
            "Epoch: 640, Loss: 0.1962\n",
            "Epoch: 641, Loss: 0.1960\n",
            "Epoch: 642, Loss: 0.1957\n",
            "Epoch: 643, Loss: 0.1955\n",
            "Epoch: 644, Loss: 0.1952\n",
            "Epoch: 645, Loss: 0.1950\n",
            "Epoch: 646, Loss: 0.1948\n",
            "Epoch: 647, Loss: 0.1945\n",
            "Epoch: 648, Loss: 0.1943\n",
            "Epoch: 649, Loss: 0.1940\n",
            "Epoch: 650, Loss: 0.1938\n",
            "Epoch: 651, Loss: 0.1935\n",
            "Epoch: 652, Loss: 0.1933\n",
            "Epoch: 653, Loss: 0.1931\n",
            "Epoch: 654, Loss: 0.1928\n",
            "Epoch: 655, Loss: 0.1926\n",
            "Epoch: 656, Loss: 0.1924\n",
            "Epoch: 657, Loss: 0.1921\n",
            "Epoch: 658, Loss: 0.1919\n",
            "Epoch: 659, Loss: 0.1916\n",
            "Epoch: 660, Loss: 0.1914\n",
            "Epoch: 661, Loss: 0.1912\n",
            "Epoch: 662, Loss: 0.1909\n",
            "Epoch: 663, Loss: 0.1907\n",
            "Epoch: 664, Loss: 0.1905\n",
            "Epoch: 665, Loss: 0.1902\n",
            "Epoch: 666, Loss: 0.1900\n",
            "Epoch: 667, Loss: 0.1898\n",
            "Epoch: 668, Loss: 0.1895\n",
            "Epoch: 669, Loss: 0.1893\n",
            "Epoch: 670, Loss: 0.1891\n",
            "Epoch: 671, Loss: 0.1889\n",
            "Epoch: 672, Loss: 0.1886\n",
            "Epoch: 673, Loss: 0.1884\n",
            "Epoch: 674, Loss: 0.1882\n",
            "Epoch: 675, Loss: 0.1879\n",
            "Epoch: 676, Loss: 0.1877\n",
            "Epoch: 677, Loss: 0.1875\n",
            "Epoch: 678, Loss: 0.1873\n",
            "Epoch: 679, Loss: 0.1870\n",
            "Epoch: 680, Loss: 0.1868\n",
            "Epoch: 681, Loss: 0.1866\n",
            "Epoch: 682, Loss: 0.1864\n",
            "Epoch: 683, Loss: 0.1862\n",
            "Epoch: 684, Loss: 0.1859\n",
            "Epoch: 685, Loss: 0.1857\n",
            "Epoch: 686, Loss: 0.1855\n",
            "Epoch: 687, Loss: 0.1853\n",
            "Epoch: 688, Loss: 0.1850\n",
            "Epoch: 689, Loss: 0.1848\n",
            "Epoch: 690, Loss: 0.1846\n",
            "Epoch: 691, Loss: 0.1844\n",
            "Epoch: 692, Loss: 0.1842\n",
            "Epoch: 693, Loss: 0.1840\n",
            "Epoch: 694, Loss: 0.1837\n",
            "Epoch: 695, Loss: 0.1835\n",
            "Epoch: 696, Loss: 0.1833\n",
            "Epoch: 697, Loss: 0.1831\n",
            "Epoch: 698, Loss: 0.1829\n",
            "Epoch: 699, Loss: 0.1827\n",
            "Epoch: 700, Loss: 0.1825\n",
            "Epoch: 701, Loss: 0.1822\n",
            "Epoch: 702, Loss: 0.1820\n",
            "Epoch: 703, Loss: 0.1818\n",
            "Epoch: 704, Loss: 0.1816\n",
            "Epoch: 705, Loss: 0.1814\n",
            "Epoch: 706, Loss: 0.1812\n",
            "Epoch: 707, Loss: 0.1810\n",
            "Epoch: 708, Loss: 0.1808\n",
            "Epoch: 709, Loss: 0.1806\n",
            "Epoch: 710, Loss: 0.1804\n",
            "Epoch: 711, Loss: 0.1802\n",
            "Epoch: 712, Loss: 0.1799\n",
            "Epoch: 713, Loss: 0.1797\n",
            "Epoch: 714, Loss: 0.1795\n",
            "Epoch: 715, Loss: 0.1793\n",
            "Epoch: 716, Loss: 0.1791\n",
            "Epoch: 717, Loss: 0.1789\n",
            "Epoch: 718, Loss: 0.1787\n",
            "Epoch: 719, Loss: 0.1785\n",
            "Epoch: 720, Loss: 0.1783\n",
            "Epoch: 721, Loss: 0.1781\n",
            "Epoch: 722, Loss: 0.1779\n",
            "Epoch: 723, Loss: 0.1777\n",
            "Epoch: 724, Loss: 0.1775\n",
            "Epoch: 725, Loss: 0.1773\n",
            "Epoch: 726, Loss: 0.1771\n",
            "Epoch: 727, Loss: 0.1769\n",
            "Epoch: 728, Loss: 0.1767\n",
            "Epoch: 729, Loss: 0.1765\n",
            "Epoch: 730, Loss: 0.1763\n",
            "Epoch: 731, Loss: 0.1761\n",
            "Epoch: 732, Loss: 0.1759\n",
            "Epoch: 733, Loss: 0.1757\n",
            "Epoch: 734, Loss: 0.1755\n",
            "Epoch: 735, Loss: 0.1753\n",
            "Epoch: 736, Loss: 0.1752\n",
            "Epoch: 737, Loss: 0.1750\n",
            "Epoch: 738, Loss: 0.1748\n",
            "Epoch: 739, Loss: 0.1746\n",
            "Epoch: 740, Loss: 0.1744\n",
            "Epoch: 741, Loss: 0.1742\n",
            "Epoch: 742, Loss: 0.1740\n",
            "Epoch: 743, Loss: 0.1738\n",
            "Epoch: 744, Loss: 0.1736\n",
            "Epoch: 745, Loss: 0.1734\n",
            "Epoch: 746, Loss: 0.1732\n",
            "Epoch: 747, Loss: 0.1731\n",
            "Epoch: 748, Loss: 0.1729\n",
            "Epoch: 749, Loss: 0.1727\n",
            "Epoch: 750, Loss: 0.1725\n",
            "Epoch: 751, Loss: 0.1723\n",
            "Epoch: 752, Loss: 0.1721\n",
            "Epoch: 753, Loss: 0.1719\n",
            "Epoch: 754, Loss: 0.1717\n",
            "Epoch: 755, Loss: 0.1716\n",
            "Epoch: 756, Loss: 0.1714\n",
            "Epoch: 757, Loss: 0.1712\n",
            "Epoch: 758, Loss: 0.1710\n",
            "Epoch: 759, Loss: 0.1708\n",
            "Epoch: 760, Loss: 0.1706\n",
            "Epoch: 761, Loss: 0.1705\n",
            "Epoch: 762, Loss: 0.1703\n",
            "Epoch: 763, Loss: 0.1701\n",
            "Epoch: 764, Loss: 0.1699\n",
            "Epoch: 765, Loss: 0.1697\n",
            "Epoch: 766, Loss: 0.1696\n",
            "Epoch: 767, Loss: 0.1694\n",
            "Epoch: 768, Loss: 0.1692\n",
            "Epoch: 769, Loss: 0.1690\n",
            "Epoch: 770, Loss: 0.1688\n",
            "Epoch: 771, Loss: 0.1687\n",
            "Epoch: 772, Loss: 0.1685\n",
            "Epoch: 773, Loss: 0.1683\n",
            "Epoch: 774, Loss: 0.1681\n",
            "Epoch: 775, Loss: 0.1680\n",
            "Epoch: 776, Loss: 0.1678\n",
            "Epoch: 777, Loss: 0.1676\n",
            "Epoch: 778, Loss: 0.1674\n",
            "Epoch: 779, Loss: 0.1673\n",
            "Epoch: 780, Loss: 0.1671\n",
            "Epoch: 781, Loss: 0.1669\n",
            "Epoch: 782, Loss: 0.1667\n",
            "Epoch: 783, Loss: 0.1666\n",
            "Epoch: 784, Loss: 0.1664\n",
            "Epoch: 785, Loss: 0.1662\n",
            "Epoch: 786, Loss: 0.1661\n",
            "Epoch: 787, Loss: 0.1659\n",
            "Epoch: 788, Loss: 0.1657\n",
            "Epoch: 789, Loss: 0.1655\n",
            "Epoch: 790, Loss: 0.1654\n",
            "Epoch: 791, Loss: 0.1652\n",
            "Epoch: 792, Loss: 0.1650\n",
            "Epoch: 793, Loss: 0.1649\n",
            "Epoch: 794, Loss: 0.1647\n",
            "Epoch: 795, Loss: 0.1645\n",
            "Epoch: 796, Loss: 0.1644\n",
            "Epoch: 797, Loss: 0.1642\n",
            "Epoch: 798, Loss: 0.1640\n",
            "Epoch: 799, Loss: 0.1639\n",
            "Epoch: 800, Loss: 0.1637\n",
            "Epoch: 801, Loss: 0.1635\n",
            "Epoch: 802, Loss: 0.1634\n",
            "Epoch: 803, Loss: 0.1632\n",
            "Epoch: 804, Loss: 0.1631\n",
            "Epoch: 805, Loss: 0.1629\n",
            "Epoch: 806, Loss: 0.1627\n",
            "Epoch: 807, Loss: 0.1626\n",
            "Epoch: 808, Loss: 0.1624\n",
            "Epoch: 809, Loss: 0.1622\n",
            "Epoch: 810, Loss: 0.1621\n",
            "Epoch: 811, Loss: 0.1619\n",
            "Epoch: 812, Loss: 0.1618\n",
            "Epoch: 813, Loss: 0.1616\n",
            "Epoch: 814, Loss: 0.1614\n",
            "Epoch: 815, Loss: 0.1613\n",
            "Epoch: 816, Loss: 0.1611\n",
            "Epoch: 817, Loss: 0.1610\n",
            "Epoch: 818, Loss: 0.1608\n",
            "Epoch: 819, Loss: 0.1607\n",
            "Epoch: 820, Loss: 0.1605\n",
            "Epoch: 821, Loss: 0.1603\n",
            "Epoch: 822, Loss: 0.1602\n",
            "Epoch: 823, Loss: 0.1600\n",
            "Epoch: 824, Loss: 0.1599\n",
            "Epoch: 825, Loss: 0.1597\n",
            "Epoch: 826, Loss: 0.1596\n",
            "Epoch: 827, Loss: 0.1594\n",
            "Epoch: 828, Loss: 0.1593\n",
            "Epoch: 829, Loss: 0.1591\n",
            "Epoch: 830, Loss: 0.1590\n",
            "Epoch: 831, Loss: 0.1588\n",
            "Epoch: 832, Loss: 0.1587\n",
            "Epoch: 833, Loss: 0.1585\n",
            "Epoch: 834, Loss: 0.1584\n",
            "Epoch: 835, Loss: 0.1582\n",
            "Epoch: 836, Loss: 0.1581\n",
            "Epoch: 837, Loss: 0.1579\n",
            "Epoch: 838, Loss: 0.1578\n",
            "Epoch: 839, Loss: 0.1576\n",
            "Epoch: 840, Loss: 0.1575\n",
            "Epoch: 841, Loss: 0.1573\n",
            "Epoch: 842, Loss: 0.1572\n",
            "Epoch: 843, Loss: 0.1570\n",
            "Epoch: 844, Loss: 0.1569\n",
            "Epoch: 845, Loss: 0.1567\n",
            "Epoch: 846, Loss: 0.1566\n",
            "Epoch: 847, Loss: 0.1564\n",
            "Epoch: 848, Loss: 0.1563\n",
            "Epoch: 849, Loss: 0.1561\n",
            "Epoch: 850, Loss: 0.1560\n",
            "Epoch: 851, Loss: 0.1558\n",
            "Epoch: 852, Loss: 0.1557\n",
            "Epoch: 853, Loss: 0.1556\n",
            "Epoch: 854, Loss: 0.1554\n",
            "Epoch: 855, Loss: 0.1553\n",
            "Epoch: 856, Loss: 0.1551\n",
            "Epoch: 857, Loss: 0.1550\n",
            "Epoch: 858, Loss: 0.1548\n",
            "Epoch: 859, Loss: 0.1547\n",
            "Epoch: 860, Loss: 0.1546\n",
            "Epoch: 861, Loss: 0.1544\n",
            "Epoch: 862, Loss: 0.1543\n",
            "Epoch: 863, Loss: 0.1541\n",
            "Epoch: 864, Loss: 0.1540\n",
            "Epoch: 865, Loss: 0.1539\n",
            "Epoch: 866, Loss: 0.1537\n",
            "Epoch: 867, Loss: 0.1536\n",
            "Epoch: 868, Loss: 0.1534\n",
            "Epoch: 869, Loss: 0.1533\n",
            "Epoch: 870, Loss: 0.1532\n",
            "Epoch: 871, Loss: 0.1530\n",
            "Epoch: 872, Loss: 0.1529\n",
            "Epoch: 873, Loss: 0.1528\n",
            "Epoch: 874, Loss: 0.1526\n",
            "Epoch: 875, Loss: 0.1525\n",
            "Epoch: 876, Loss: 0.1523\n",
            "Epoch: 877, Loss: 0.1522\n",
            "Epoch: 878, Loss: 0.1521\n",
            "Epoch: 879, Loss: 0.1519\n",
            "Epoch: 880, Loss: 0.1518\n",
            "Epoch: 881, Loss: 0.1517\n",
            "Epoch: 882, Loss: 0.1515\n",
            "Epoch: 883, Loss: 0.1514\n",
            "Epoch: 884, Loss: 0.1513\n",
            "Epoch: 885, Loss: 0.1511\n",
            "Epoch: 886, Loss: 0.1510\n",
            "Epoch: 887, Loss: 0.1509\n",
            "Epoch: 888, Loss: 0.1507\n",
            "Epoch: 889, Loss: 0.1506\n",
            "Epoch: 890, Loss: 0.1505\n",
            "Epoch: 891, Loss: 0.1503\n",
            "Epoch: 892, Loss: 0.1502\n",
            "Epoch: 893, Loss: 0.1501\n",
            "Epoch: 894, Loss: 0.1500\n",
            "Epoch: 895, Loss: 0.1498\n",
            "Epoch: 896, Loss: 0.1497\n",
            "Epoch: 897, Loss: 0.1496\n",
            "Epoch: 898, Loss: 0.1494\n",
            "Epoch: 899, Loss: 0.1493\n",
            "Epoch: 900, Loss: 0.1492\n",
            "Epoch: 901, Loss: 0.1491\n",
            "Epoch: 902, Loss: 0.1489\n",
            "Epoch: 903, Loss: 0.1488\n",
            "Epoch: 904, Loss: 0.1487\n",
            "Epoch: 905, Loss: 0.1486\n",
            "Epoch: 906, Loss: 0.1484\n",
            "Epoch: 907, Loss: 0.1483\n",
            "Epoch: 908, Loss: 0.1482\n",
            "Epoch: 909, Loss: 0.1481\n",
            "Epoch: 910, Loss: 0.1479\n",
            "Epoch: 911, Loss: 0.1478\n",
            "Epoch: 912, Loss: 0.1477\n",
            "Epoch: 913, Loss: 0.1476\n",
            "Epoch: 914, Loss: 0.1474\n",
            "Epoch: 915, Loss: 0.1473\n",
            "Epoch: 916, Loss: 0.1472\n",
            "Epoch: 917, Loss: 0.1471\n",
            "Epoch: 918, Loss: 0.1469\n",
            "Epoch: 919, Loss: 0.1468\n",
            "Epoch: 920, Loss: 0.1467\n",
            "Epoch: 921, Loss: 0.1466\n",
            "Epoch: 922, Loss: 0.1465\n",
            "Epoch: 923, Loss: 0.1463\n",
            "Epoch: 924, Loss: 0.1462\n",
            "Epoch: 925, Loss: 0.1461\n",
            "Epoch: 926, Loss: 0.1460\n",
            "Epoch: 927, Loss: 0.1459\n",
            "Epoch: 928, Loss: 0.1457\n",
            "Epoch: 929, Loss: 0.1456\n",
            "Epoch: 930, Loss: 0.1455\n",
            "Epoch: 931, Loss: 0.1454\n",
            "Epoch: 932, Loss: 0.1453\n",
            "Epoch: 933, Loss: 0.1452\n",
            "Epoch: 934, Loss: 0.1450\n",
            "Epoch: 935, Loss: 0.1449\n",
            "Epoch: 936, Loss: 0.1448\n",
            "Epoch: 937, Loss: 0.1447\n",
            "Epoch: 938, Loss: 0.1446\n",
            "Epoch: 939, Loss: 0.1445\n",
            "Epoch: 940, Loss: 0.1443\n",
            "Epoch: 941, Loss: 0.1442\n",
            "Epoch: 942, Loss: 0.1441\n",
            "Epoch: 943, Loss: 0.1440\n",
            "Epoch: 944, Loss: 0.1439\n",
            "Epoch: 945, Loss: 0.1438\n",
            "Epoch: 946, Loss: 0.1437\n",
            "Epoch: 947, Loss: 0.1435\n",
            "Epoch: 948, Loss: 0.1434\n",
            "Epoch: 949, Loss: 0.1433\n",
            "Epoch: 950, Loss: 0.1432\n",
            "Epoch: 951, Loss: 0.1431\n",
            "Epoch: 952, Loss: 0.1430\n",
            "Epoch: 953, Loss: 0.1429\n",
            "Epoch: 954, Loss: 0.1428\n",
            "Epoch: 955, Loss: 0.1426\n",
            "Epoch: 956, Loss: 0.1425\n",
            "Epoch: 957, Loss: 0.1424\n",
            "Epoch: 958, Loss: 0.1423\n",
            "Epoch: 959, Loss: 0.1422\n",
            "Epoch: 960, Loss: 0.1421\n",
            "Epoch: 961, Loss: 0.1420\n",
            "Epoch: 962, Loss: 0.1419\n",
            "Epoch: 963, Loss: 0.1418\n",
            "Epoch: 964, Loss: 0.1417\n",
            "Epoch: 965, Loss: 0.1416\n",
            "Epoch: 966, Loss: 0.1414\n",
            "Epoch: 967, Loss: 0.1413\n",
            "Epoch: 968, Loss: 0.1412\n",
            "Epoch: 969, Loss: 0.1411\n",
            "Epoch: 970, Loss: 0.1410\n",
            "Epoch: 971, Loss: 0.1409\n",
            "Epoch: 972, Loss: 0.1408\n",
            "Epoch: 973, Loss: 0.1407\n",
            "Epoch: 974, Loss: 0.1406\n",
            "Epoch: 975, Loss: 0.1405\n",
            "Epoch: 976, Loss: 0.1404\n",
            "Epoch: 977, Loss: 0.1403\n",
            "Epoch: 978, Loss: 0.1402\n",
            "Epoch: 979, Loss: 0.1401\n",
            "Epoch: 980, Loss: 0.1400\n",
            "Epoch: 981, Loss: 0.1399\n",
            "Epoch: 982, Loss: 0.1398\n",
            "Epoch: 983, Loss: 0.1396\n",
            "Epoch: 984, Loss: 0.1395\n",
            "Epoch: 985, Loss: 0.1394\n",
            "Epoch: 986, Loss: 0.1393\n",
            "Epoch: 987, Loss: 0.1392\n",
            "Epoch: 988, Loss: 0.1391\n",
            "Epoch: 989, Loss: 0.1390\n",
            "Epoch: 990, Loss: 0.1389\n",
            "Epoch: 991, Loss: 0.1388\n",
            "Epoch: 992, Loss: 0.1387\n",
            "Epoch: 993, Loss: 0.1386\n",
            "Epoch: 994, Loss: 0.1385\n",
            "Epoch: 995, Loss: 0.1384\n",
            "Epoch: 996, Loss: 0.1383\n",
            "Epoch: 997, Loss: 0.1382\n",
            "Epoch: 998, Loss: 0.1381\n",
            "Epoch: 999, Loss: 0.1380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQQOOvQpnhsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b497c0f-f447-4fec-9a34-acd786b60f45"
      },
      "source": [
        "for param in model.parameters():\n",
        "    print(param.data) # 很接近 3,5,1"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3.3360, 4.4918, 1.9250]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wcqrFHnoa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as opt"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDLVT4ULn-B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = opt.SGD(model.parameters(), lr=leaning_rate) # 接近上面最基礎的 Gradient Desent"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuGWTA96o2M5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "532df686-bd4e-4842-9723-69d40e1a93e5"
      },
      "source": [
        "for epoch in range(epoch_n * 10): # epoch 10倍\n",
        "  y_predict = model(Vx)\n",
        "  loss = loss_func(y_predict, y) # y_predict,y的順序要注意\n",
        "  print(\"Epoch: {}, Loss: {:.4f}\".format(epoch,loss))\n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 32.7080\n",
            "Epoch: 1, Loss: 31.6028\n",
            "Epoch: 2, Loss: 30.5356\n",
            "Epoch: 3, Loss: 29.5049\n",
            "Epoch: 4, Loss: 28.5095\n",
            "Epoch: 5, Loss: 27.5483\n",
            "Epoch: 6, Loss: 26.6200\n",
            "Epoch: 7, Loss: 25.7236\n",
            "Epoch: 8, Loss: 24.8578\n",
            "Epoch: 9, Loss: 24.0217\n",
            "Epoch: 10, Loss: 23.2143\n",
            "Epoch: 11, Loss: 22.4346\n",
            "Epoch: 12, Loss: 21.6815\n",
            "Epoch: 13, Loss: 20.9543\n",
            "Epoch: 14, Loss: 20.2520\n",
            "Epoch: 15, Loss: 19.5738\n",
            "Epoch: 16, Loss: 18.9188\n",
            "Epoch: 17, Loss: 18.2862\n",
            "Epoch: 18, Loss: 17.6753\n",
            "Epoch: 19, Loss: 17.0854\n",
            "Epoch: 20, Loss: 16.5156\n",
            "Epoch: 21, Loss: 15.9654\n",
            "Epoch: 22, Loss: 15.4340\n",
            "Epoch: 23, Loss: 14.9208\n",
            "Epoch: 24, Loss: 14.4251\n",
            "Epoch: 25, Loss: 13.9465\n",
            "Epoch: 26, Loss: 13.4842\n",
            "Epoch: 27, Loss: 13.0378\n",
            "Epoch: 28, Loss: 12.6067\n",
            "Epoch: 29, Loss: 12.1903\n",
            "Epoch: 30, Loss: 11.7882\n",
            "Epoch: 31, Loss: 11.3998\n",
            "Epoch: 32, Loss: 11.0247\n",
            "Epoch: 33, Loss: 10.6625\n",
            "Epoch: 34, Loss: 10.3127\n",
            "Epoch: 35, Loss: 9.9748\n",
            "Epoch: 36, Loss: 9.6485\n",
            "Epoch: 37, Loss: 9.3333\n",
            "Epoch: 38, Loss: 9.0290\n",
            "Epoch: 39, Loss: 8.7350\n",
            "Epoch: 40, Loss: 8.4511\n",
            "Epoch: 41, Loss: 8.1769\n",
            "Epoch: 42, Loss: 7.9121\n",
            "Epoch: 43, Loss: 7.6564\n",
            "Epoch: 44, Loss: 7.4094\n",
            "Epoch: 45, Loss: 7.1708\n",
            "Epoch: 46, Loss: 6.9404\n",
            "Epoch: 47, Loss: 6.7178\n",
            "Epoch: 48, Loss: 6.5029\n",
            "Epoch: 49, Loss: 6.2953\n",
            "Epoch: 50, Loss: 6.0948\n",
            "Epoch: 51, Loss: 5.9011\n",
            "Epoch: 52, Loss: 5.7141\n",
            "Epoch: 53, Loss: 5.5335\n",
            "Epoch: 54, Loss: 5.3590\n",
            "Epoch: 55, Loss: 5.1904\n",
            "Epoch: 56, Loss: 5.0277\n",
            "Epoch: 57, Loss: 4.8704\n",
            "Epoch: 58, Loss: 4.7186\n",
            "Epoch: 59, Loss: 4.5719\n",
            "Epoch: 60, Loss: 4.4302\n",
            "Epoch: 61, Loss: 4.2934\n",
            "Epoch: 62, Loss: 4.1612\n",
            "Epoch: 63, Loss: 4.0335\n",
            "Epoch: 64, Loss: 3.9102\n",
            "Epoch: 65, Loss: 3.7911\n",
            "Epoch: 66, Loss: 3.6760\n",
            "Epoch: 67, Loss: 3.5649\n",
            "Epoch: 68, Loss: 3.4575\n",
            "Epoch: 69, Loss: 3.3538\n",
            "Epoch: 70, Loss: 3.2536\n",
            "Epoch: 71, Loss: 3.1568\n",
            "Epoch: 72, Loss: 3.0633\n",
            "Epoch: 73, Loss: 2.9730\n",
            "Epoch: 74, Loss: 2.8858\n",
            "Epoch: 75, Loss: 2.8015\n",
            "Epoch: 76, Loss: 2.7201\n",
            "Epoch: 77, Loss: 2.6415\n",
            "Epoch: 78, Loss: 2.5655\n",
            "Epoch: 79, Loss: 2.4921\n",
            "Epoch: 80, Loss: 2.4212\n",
            "Epoch: 81, Loss: 2.3527\n",
            "Epoch: 82, Loss: 2.2865\n",
            "Epoch: 83, Loss: 2.2226\n",
            "Epoch: 84, Loss: 2.1608\n",
            "Epoch: 85, Loss: 2.1011\n",
            "Epoch: 86, Loss: 2.0434\n",
            "Epoch: 87, Loss: 1.9877\n",
            "Epoch: 88, Loss: 1.9339\n",
            "Epoch: 89, Loss: 1.8819\n",
            "Epoch: 90, Loss: 1.8316\n",
            "Epoch: 91, Loss: 1.7830\n",
            "Epoch: 92, Loss: 1.7361\n",
            "Epoch: 93, Loss: 1.6908\n",
            "Epoch: 94, Loss: 1.6470\n",
            "Epoch: 95, Loss: 1.6046\n",
            "Epoch: 96, Loss: 1.5637\n",
            "Epoch: 97, Loss: 1.5242\n",
            "Epoch: 98, Loss: 1.4860\n",
            "Epoch: 99, Loss: 1.4490\n",
            "Epoch: 100, Loss: 1.4134\n",
            "Epoch: 101, Loss: 1.3789\n",
            "Epoch: 102, Loss: 1.3455\n",
            "Epoch: 103, Loss: 1.3133\n",
            "Epoch: 104, Loss: 1.2822\n",
            "Epoch: 105, Loss: 1.2521\n",
            "Epoch: 106, Loss: 1.2230\n",
            "Epoch: 107, Loss: 1.1949\n",
            "Epoch: 108, Loss: 1.1677\n",
            "Epoch: 109, Loss: 1.1414\n",
            "Epoch: 110, Loss: 1.1160\n",
            "Epoch: 111, Loss: 1.0914\n",
            "Epoch: 112, Loss: 1.0677\n",
            "Epoch: 113, Loss: 1.0448\n",
            "Epoch: 114, Loss: 1.0226\n",
            "Epoch: 115, Loss: 1.0011\n",
            "Epoch: 116, Loss: 0.9804\n",
            "Epoch: 117, Loss: 0.9603\n",
            "Epoch: 118, Loss: 0.9409\n",
            "Epoch: 119, Loss: 0.9221\n",
            "Epoch: 120, Loss: 0.9040\n",
            "Epoch: 121, Loss: 0.8864\n",
            "Epoch: 122, Loss: 0.8695\n",
            "Epoch: 123, Loss: 0.8531\n",
            "Epoch: 124, Loss: 0.8372\n",
            "Epoch: 125, Loss: 0.8218\n",
            "Epoch: 126, Loss: 0.8070\n",
            "Epoch: 127, Loss: 0.7926\n",
            "Epoch: 128, Loss: 0.7787\n",
            "Epoch: 129, Loss: 0.7652\n",
            "Epoch: 130, Loss: 0.7522\n",
            "Epoch: 131, Loss: 0.7396\n",
            "Epoch: 132, Loss: 0.7274\n",
            "Epoch: 133, Loss: 0.7156\n",
            "Epoch: 134, Loss: 0.7042\n",
            "Epoch: 135, Loss: 0.6932\n",
            "Epoch: 136, Loss: 0.6825\n",
            "Epoch: 137, Loss: 0.6721\n",
            "Epoch: 138, Loss: 0.6621\n",
            "Epoch: 139, Loss: 0.6524\n",
            "Epoch: 140, Loss: 0.6430\n",
            "Epoch: 141, Loss: 0.6339\n",
            "Epoch: 142, Loss: 0.6251\n",
            "Epoch: 143, Loss: 0.6165\n",
            "Epoch: 144, Loss: 0.6083\n",
            "Epoch: 145, Loss: 0.6003\n",
            "Epoch: 146, Loss: 0.5925\n",
            "Epoch: 147, Loss: 0.5850\n",
            "Epoch: 148, Loss: 0.5777\n",
            "Epoch: 149, Loss: 0.5706\n",
            "Epoch: 150, Loss: 0.5638\n",
            "Epoch: 151, Loss: 0.5572\n",
            "Epoch: 152, Loss: 0.5507\n",
            "Epoch: 153, Loss: 0.5445\n",
            "Epoch: 154, Loss: 0.5385\n",
            "Epoch: 155, Loss: 0.5326\n",
            "Epoch: 156, Loss: 0.5269\n",
            "Epoch: 157, Loss: 0.5214\n",
            "Epoch: 158, Loss: 0.5161\n",
            "Epoch: 159, Loss: 0.5109\n",
            "Epoch: 160, Loss: 0.5059\n",
            "Epoch: 161, Loss: 0.5010\n",
            "Epoch: 162, Loss: 0.4963\n",
            "Epoch: 163, Loss: 0.4917\n",
            "Epoch: 164, Loss: 0.4872\n",
            "Epoch: 165, Loss: 0.4829\n",
            "Epoch: 166, Loss: 0.4787\n",
            "Epoch: 167, Loss: 0.4746\n",
            "Epoch: 168, Loss: 0.4706\n",
            "Epoch: 169, Loss: 0.4668\n",
            "Epoch: 170, Loss: 0.4630\n",
            "Epoch: 171, Loss: 0.4594\n",
            "Epoch: 172, Loss: 0.4558\n",
            "Epoch: 173, Loss: 0.4524\n",
            "Epoch: 174, Loss: 0.4491\n",
            "Epoch: 175, Loss: 0.4458\n",
            "Epoch: 176, Loss: 0.4426\n",
            "Epoch: 177, Loss: 0.4396\n",
            "Epoch: 178, Loss: 0.4366\n",
            "Epoch: 179, Loss: 0.4337\n",
            "Epoch: 180, Loss: 0.4308\n",
            "Epoch: 181, Loss: 0.4281\n",
            "Epoch: 182, Loss: 0.4254\n",
            "Epoch: 183, Loss: 0.4227\n",
            "Epoch: 184, Loss: 0.4202\n",
            "Epoch: 185, Loss: 0.4177\n",
            "Epoch: 186, Loss: 0.4153\n",
            "Epoch: 187, Loss: 0.4129\n",
            "Epoch: 188, Loss: 0.4106\n",
            "Epoch: 189, Loss: 0.4084\n",
            "Epoch: 190, Loss: 0.4062\n",
            "Epoch: 191, Loss: 0.4040\n",
            "Epoch: 192, Loss: 0.4020\n",
            "Epoch: 193, Loss: 0.3999\n",
            "Epoch: 194, Loss: 0.3979\n",
            "Epoch: 195, Loss: 0.3960\n",
            "Epoch: 196, Loss: 0.3941\n",
            "Epoch: 197, Loss: 0.3922\n",
            "Epoch: 198, Loss: 0.3904\n",
            "Epoch: 199, Loss: 0.3887\n",
            "Epoch: 200, Loss: 0.3869\n",
            "Epoch: 201, Loss: 0.3852\n",
            "Epoch: 202, Loss: 0.3836\n",
            "Epoch: 203, Loss: 0.3820\n",
            "Epoch: 204, Loss: 0.3804\n",
            "Epoch: 205, Loss: 0.3788\n",
            "Epoch: 206, Loss: 0.3773\n",
            "Epoch: 207, Loss: 0.3758\n",
            "Epoch: 208, Loss: 0.3744\n",
            "Epoch: 209, Loss: 0.3729\n",
            "Epoch: 210, Loss: 0.3715\n",
            "Epoch: 211, Loss: 0.3702\n",
            "Epoch: 212, Loss: 0.3688\n",
            "Epoch: 213, Loss: 0.3675\n",
            "Epoch: 214, Loss: 0.3662\n",
            "Epoch: 215, Loss: 0.3649\n",
            "Epoch: 216, Loss: 0.3637\n",
            "Epoch: 217, Loss: 0.3625\n",
            "Epoch: 218, Loss: 0.3613\n",
            "Epoch: 219, Loss: 0.3601\n",
            "Epoch: 220, Loss: 0.3589\n",
            "Epoch: 221, Loss: 0.3578\n",
            "Epoch: 222, Loss: 0.3566\n",
            "Epoch: 223, Loss: 0.3555\n",
            "Epoch: 224, Loss: 0.3544\n",
            "Epoch: 225, Loss: 0.3534\n",
            "Epoch: 226, Loss: 0.3523\n",
            "Epoch: 227, Loss: 0.3513\n",
            "Epoch: 228, Loss: 0.3503\n",
            "Epoch: 229, Loss: 0.3493\n",
            "Epoch: 230, Loss: 0.3483\n",
            "Epoch: 231, Loss: 0.3473\n",
            "Epoch: 232, Loss: 0.3463\n",
            "Epoch: 233, Loss: 0.3454\n",
            "Epoch: 234, Loss: 0.3444\n",
            "Epoch: 235, Loss: 0.3435\n",
            "Epoch: 236, Loss: 0.3426\n",
            "Epoch: 237, Loss: 0.3417\n",
            "Epoch: 238, Loss: 0.3408\n",
            "Epoch: 239, Loss: 0.3399\n",
            "Epoch: 240, Loss: 0.3391\n",
            "Epoch: 241, Loss: 0.3382\n",
            "Epoch: 242, Loss: 0.3374\n",
            "Epoch: 243, Loss: 0.3365\n",
            "Epoch: 244, Loss: 0.3357\n",
            "Epoch: 245, Loss: 0.3349\n",
            "Epoch: 246, Loss: 0.3341\n",
            "Epoch: 247, Loss: 0.3333\n",
            "Epoch: 248, Loss: 0.3325\n",
            "Epoch: 249, Loss: 0.3317\n",
            "Epoch: 250, Loss: 0.3309\n",
            "Epoch: 251, Loss: 0.3302\n",
            "Epoch: 252, Loss: 0.3294\n",
            "Epoch: 253, Loss: 0.3287\n",
            "Epoch: 254, Loss: 0.3279\n",
            "Epoch: 255, Loss: 0.3272\n",
            "Epoch: 256, Loss: 0.3265\n",
            "Epoch: 257, Loss: 0.3257\n",
            "Epoch: 258, Loss: 0.3250\n",
            "Epoch: 259, Loss: 0.3243\n",
            "Epoch: 260, Loss: 0.3236\n",
            "Epoch: 261, Loss: 0.3229\n",
            "Epoch: 262, Loss: 0.3222\n",
            "Epoch: 263, Loss: 0.3215\n",
            "Epoch: 264, Loss: 0.3209\n",
            "Epoch: 265, Loss: 0.3202\n",
            "Epoch: 266, Loss: 0.3195\n",
            "Epoch: 267, Loss: 0.3189\n",
            "Epoch: 268, Loss: 0.3182\n",
            "Epoch: 269, Loss: 0.3175\n",
            "Epoch: 270, Loss: 0.3169\n",
            "Epoch: 271, Loss: 0.3162\n",
            "Epoch: 272, Loss: 0.3156\n",
            "Epoch: 273, Loss: 0.3150\n",
            "Epoch: 274, Loss: 0.3143\n",
            "Epoch: 275, Loss: 0.3137\n",
            "Epoch: 276, Loss: 0.3131\n",
            "Epoch: 277, Loss: 0.3125\n",
            "Epoch: 278, Loss: 0.3118\n",
            "Epoch: 279, Loss: 0.3112\n",
            "Epoch: 280, Loss: 0.3106\n",
            "Epoch: 281, Loss: 0.3100\n",
            "Epoch: 282, Loss: 0.3094\n",
            "Epoch: 283, Loss: 0.3088\n",
            "Epoch: 284, Loss: 0.3082\n",
            "Epoch: 285, Loss: 0.3076\n",
            "Epoch: 286, Loss: 0.3070\n",
            "Epoch: 287, Loss: 0.3065\n",
            "Epoch: 288, Loss: 0.3059\n",
            "Epoch: 289, Loss: 0.3053\n",
            "Epoch: 290, Loss: 0.3047\n",
            "Epoch: 291, Loss: 0.3041\n",
            "Epoch: 292, Loss: 0.3036\n",
            "Epoch: 293, Loss: 0.3030\n",
            "Epoch: 294, Loss: 0.3024\n",
            "Epoch: 295, Loss: 0.3019\n",
            "Epoch: 296, Loss: 0.3013\n",
            "Epoch: 297, Loss: 0.3008\n",
            "Epoch: 298, Loss: 0.3002\n",
            "Epoch: 299, Loss: 0.2997\n",
            "Epoch: 300, Loss: 0.2991\n",
            "Epoch: 301, Loss: 0.2986\n",
            "Epoch: 302, Loss: 0.2980\n",
            "Epoch: 303, Loss: 0.2975\n",
            "Epoch: 304, Loss: 0.2969\n",
            "Epoch: 305, Loss: 0.2964\n",
            "Epoch: 306, Loss: 0.2959\n",
            "Epoch: 307, Loss: 0.2953\n",
            "Epoch: 308, Loss: 0.2948\n",
            "Epoch: 309, Loss: 0.2943\n",
            "Epoch: 310, Loss: 0.2938\n",
            "Epoch: 311, Loss: 0.2932\n",
            "Epoch: 312, Loss: 0.2927\n",
            "Epoch: 313, Loss: 0.2922\n",
            "Epoch: 314, Loss: 0.2917\n",
            "Epoch: 315, Loss: 0.2912\n",
            "Epoch: 316, Loss: 0.2906\n",
            "Epoch: 317, Loss: 0.2901\n",
            "Epoch: 318, Loss: 0.2896\n",
            "Epoch: 319, Loss: 0.2891\n",
            "Epoch: 320, Loss: 0.2886\n",
            "Epoch: 321, Loss: 0.2881\n",
            "Epoch: 322, Loss: 0.2876\n",
            "Epoch: 323, Loss: 0.2871\n",
            "Epoch: 324, Loss: 0.2866\n",
            "Epoch: 325, Loss: 0.2861\n",
            "Epoch: 326, Loss: 0.2856\n",
            "Epoch: 327, Loss: 0.2851\n",
            "Epoch: 328, Loss: 0.2846\n",
            "Epoch: 329, Loss: 0.2841\n",
            "Epoch: 330, Loss: 0.2836\n",
            "Epoch: 331, Loss: 0.2832\n",
            "Epoch: 332, Loss: 0.2827\n",
            "Epoch: 333, Loss: 0.2822\n",
            "Epoch: 334, Loss: 0.2817\n",
            "Epoch: 335, Loss: 0.2812\n",
            "Epoch: 336, Loss: 0.2807\n",
            "Epoch: 337, Loss: 0.2803\n",
            "Epoch: 338, Loss: 0.2798\n",
            "Epoch: 339, Loss: 0.2793\n",
            "Epoch: 340, Loss: 0.2788\n",
            "Epoch: 341, Loss: 0.2784\n",
            "Epoch: 342, Loss: 0.2779\n",
            "Epoch: 343, Loss: 0.2774\n",
            "Epoch: 344, Loss: 0.2770\n",
            "Epoch: 345, Loss: 0.2765\n",
            "Epoch: 346, Loss: 0.2760\n",
            "Epoch: 347, Loss: 0.2756\n",
            "Epoch: 348, Loss: 0.2751\n",
            "Epoch: 349, Loss: 0.2746\n",
            "Epoch: 350, Loss: 0.2742\n",
            "Epoch: 351, Loss: 0.2737\n",
            "Epoch: 352, Loss: 0.2733\n",
            "Epoch: 353, Loss: 0.2728\n",
            "Epoch: 354, Loss: 0.2724\n",
            "Epoch: 355, Loss: 0.2719\n",
            "Epoch: 356, Loss: 0.2715\n",
            "Epoch: 357, Loss: 0.2710\n",
            "Epoch: 358, Loss: 0.2706\n",
            "Epoch: 359, Loss: 0.2701\n",
            "Epoch: 360, Loss: 0.2697\n",
            "Epoch: 361, Loss: 0.2692\n",
            "Epoch: 362, Loss: 0.2688\n",
            "Epoch: 363, Loss: 0.2683\n",
            "Epoch: 364, Loss: 0.2679\n",
            "Epoch: 365, Loss: 0.2675\n",
            "Epoch: 366, Loss: 0.2670\n",
            "Epoch: 367, Loss: 0.2666\n",
            "Epoch: 368, Loss: 0.2661\n",
            "Epoch: 369, Loss: 0.2657\n",
            "Epoch: 370, Loss: 0.2653\n",
            "Epoch: 371, Loss: 0.2648\n",
            "Epoch: 372, Loss: 0.2644\n",
            "Epoch: 373, Loss: 0.2640\n",
            "Epoch: 374, Loss: 0.2635\n",
            "Epoch: 375, Loss: 0.2631\n",
            "Epoch: 376, Loss: 0.2627\n",
            "Epoch: 377, Loss: 0.2623\n",
            "Epoch: 378, Loss: 0.2618\n",
            "Epoch: 379, Loss: 0.2614\n",
            "Epoch: 380, Loss: 0.2610\n",
            "Epoch: 381, Loss: 0.2606\n",
            "Epoch: 382, Loss: 0.2602\n",
            "Epoch: 383, Loss: 0.2597\n",
            "Epoch: 384, Loss: 0.2593\n",
            "Epoch: 385, Loss: 0.2589\n",
            "Epoch: 386, Loss: 0.2585\n",
            "Epoch: 387, Loss: 0.2581\n",
            "Epoch: 388, Loss: 0.2577\n",
            "Epoch: 389, Loss: 0.2573\n",
            "Epoch: 390, Loss: 0.2568\n",
            "Epoch: 391, Loss: 0.2564\n",
            "Epoch: 392, Loss: 0.2560\n",
            "Epoch: 393, Loss: 0.2556\n",
            "Epoch: 394, Loss: 0.2552\n",
            "Epoch: 395, Loss: 0.2548\n",
            "Epoch: 396, Loss: 0.2544\n",
            "Epoch: 397, Loss: 0.2540\n",
            "Epoch: 398, Loss: 0.2536\n",
            "Epoch: 399, Loss: 0.2532\n",
            "Epoch: 400, Loss: 0.2528\n",
            "Epoch: 401, Loss: 0.2524\n",
            "Epoch: 402, Loss: 0.2520\n",
            "Epoch: 403, Loss: 0.2516\n",
            "Epoch: 404, Loss: 0.2512\n",
            "Epoch: 405, Loss: 0.2508\n",
            "Epoch: 406, Loss: 0.2504\n",
            "Epoch: 407, Loss: 0.2500\n",
            "Epoch: 408, Loss: 0.2497\n",
            "Epoch: 409, Loss: 0.2493\n",
            "Epoch: 410, Loss: 0.2489\n",
            "Epoch: 411, Loss: 0.2485\n",
            "Epoch: 412, Loss: 0.2481\n",
            "Epoch: 413, Loss: 0.2477\n",
            "Epoch: 414, Loss: 0.2473\n",
            "Epoch: 415, Loss: 0.2469\n",
            "Epoch: 416, Loss: 0.2466\n",
            "Epoch: 417, Loss: 0.2462\n",
            "Epoch: 418, Loss: 0.2458\n",
            "Epoch: 419, Loss: 0.2454\n",
            "Epoch: 420, Loss: 0.2450\n",
            "Epoch: 421, Loss: 0.2447\n",
            "Epoch: 422, Loss: 0.2443\n",
            "Epoch: 423, Loss: 0.2439\n",
            "Epoch: 424, Loss: 0.2435\n",
            "Epoch: 425, Loss: 0.2432\n",
            "Epoch: 426, Loss: 0.2428\n",
            "Epoch: 427, Loss: 0.2424\n",
            "Epoch: 428, Loss: 0.2420\n",
            "Epoch: 429, Loss: 0.2417\n",
            "Epoch: 430, Loss: 0.2413\n",
            "Epoch: 431, Loss: 0.2409\n",
            "Epoch: 432, Loss: 0.2406\n",
            "Epoch: 433, Loss: 0.2402\n",
            "Epoch: 434, Loss: 0.2398\n",
            "Epoch: 435, Loss: 0.2395\n",
            "Epoch: 436, Loss: 0.2391\n",
            "Epoch: 437, Loss: 0.2388\n",
            "Epoch: 438, Loss: 0.2384\n",
            "Epoch: 439, Loss: 0.2380\n",
            "Epoch: 440, Loss: 0.2377\n",
            "Epoch: 441, Loss: 0.2373\n",
            "Epoch: 442, Loss: 0.2370\n",
            "Epoch: 443, Loss: 0.2366\n",
            "Epoch: 444, Loss: 0.2362\n",
            "Epoch: 445, Loss: 0.2359\n",
            "Epoch: 446, Loss: 0.2355\n",
            "Epoch: 447, Loss: 0.2352\n",
            "Epoch: 448, Loss: 0.2348\n",
            "Epoch: 449, Loss: 0.2345\n",
            "Epoch: 450, Loss: 0.2341\n",
            "Epoch: 451, Loss: 0.2338\n",
            "Epoch: 452, Loss: 0.2334\n",
            "Epoch: 453, Loss: 0.2331\n",
            "Epoch: 454, Loss: 0.2327\n",
            "Epoch: 455, Loss: 0.2324\n",
            "Epoch: 456, Loss: 0.2320\n",
            "Epoch: 457, Loss: 0.2317\n",
            "Epoch: 458, Loss: 0.2314\n",
            "Epoch: 459, Loss: 0.2310\n",
            "Epoch: 460, Loss: 0.2307\n",
            "Epoch: 461, Loss: 0.2303\n",
            "Epoch: 462, Loss: 0.2300\n",
            "Epoch: 463, Loss: 0.2297\n",
            "Epoch: 464, Loss: 0.2293\n",
            "Epoch: 465, Loss: 0.2290\n",
            "Epoch: 466, Loss: 0.2286\n",
            "Epoch: 467, Loss: 0.2283\n",
            "Epoch: 468, Loss: 0.2280\n",
            "Epoch: 469, Loss: 0.2276\n",
            "Epoch: 470, Loss: 0.2273\n",
            "Epoch: 471, Loss: 0.2270\n",
            "Epoch: 472, Loss: 0.2267\n",
            "Epoch: 473, Loss: 0.2263\n",
            "Epoch: 474, Loss: 0.2260\n",
            "Epoch: 475, Loss: 0.2257\n",
            "Epoch: 476, Loss: 0.2253\n",
            "Epoch: 477, Loss: 0.2250\n",
            "Epoch: 478, Loss: 0.2247\n",
            "Epoch: 479, Loss: 0.2244\n",
            "Epoch: 480, Loss: 0.2240\n",
            "Epoch: 481, Loss: 0.2237\n",
            "Epoch: 482, Loss: 0.2234\n",
            "Epoch: 483, Loss: 0.2231\n",
            "Epoch: 484, Loss: 0.2228\n",
            "Epoch: 485, Loss: 0.2224\n",
            "Epoch: 486, Loss: 0.2221\n",
            "Epoch: 487, Loss: 0.2218\n",
            "Epoch: 488, Loss: 0.2215\n",
            "Epoch: 489, Loss: 0.2212\n",
            "Epoch: 490, Loss: 0.2208\n",
            "Epoch: 491, Loss: 0.2205\n",
            "Epoch: 492, Loss: 0.2202\n",
            "Epoch: 493, Loss: 0.2199\n",
            "Epoch: 494, Loss: 0.2196\n",
            "Epoch: 495, Loss: 0.2193\n",
            "Epoch: 496, Loss: 0.2190\n",
            "Epoch: 497, Loss: 0.2187\n",
            "Epoch: 498, Loss: 0.2184\n",
            "Epoch: 499, Loss: 0.2180\n",
            "Epoch: 500, Loss: 0.2177\n",
            "Epoch: 501, Loss: 0.2174\n",
            "Epoch: 502, Loss: 0.2171\n",
            "Epoch: 503, Loss: 0.2168\n",
            "Epoch: 504, Loss: 0.2165\n",
            "Epoch: 505, Loss: 0.2162\n",
            "Epoch: 506, Loss: 0.2159\n",
            "Epoch: 507, Loss: 0.2156\n",
            "Epoch: 508, Loss: 0.2153\n",
            "Epoch: 509, Loss: 0.2150\n",
            "Epoch: 510, Loss: 0.2147\n",
            "Epoch: 511, Loss: 0.2144\n",
            "Epoch: 512, Loss: 0.2141\n",
            "Epoch: 513, Loss: 0.2138\n",
            "Epoch: 514, Loss: 0.2135\n",
            "Epoch: 515, Loss: 0.2132\n",
            "Epoch: 516, Loss: 0.2129\n",
            "Epoch: 517, Loss: 0.2126\n",
            "Epoch: 518, Loss: 0.2123\n",
            "Epoch: 519, Loss: 0.2120\n",
            "Epoch: 520, Loss: 0.2118\n",
            "Epoch: 521, Loss: 0.2115\n",
            "Epoch: 522, Loss: 0.2112\n",
            "Epoch: 523, Loss: 0.2109\n",
            "Epoch: 524, Loss: 0.2106\n",
            "Epoch: 525, Loss: 0.2103\n",
            "Epoch: 526, Loss: 0.2100\n",
            "Epoch: 527, Loss: 0.2097\n",
            "Epoch: 528, Loss: 0.2094\n",
            "Epoch: 529, Loss: 0.2092\n",
            "Epoch: 530, Loss: 0.2089\n",
            "Epoch: 531, Loss: 0.2086\n",
            "Epoch: 532, Loss: 0.2083\n",
            "Epoch: 533, Loss: 0.2080\n",
            "Epoch: 534, Loss: 0.2077\n",
            "Epoch: 535, Loss: 0.2075\n",
            "Epoch: 536, Loss: 0.2072\n",
            "Epoch: 537, Loss: 0.2069\n",
            "Epoch: 538, Loss: 0.2066\n",
            "Epoch: 539, Loss: 0.2063\n",
            "Epoch: 540, Loss: 0.2061\n",
            "Epoch: 541, Loss: 0.2058\n",
            "Epoch: 542, Loss: 0.2055\n",
            "Epoch: 543, Loss: 0.2052\n",
            "Epoch: 544, Loss: 0.2050\n",
            "Epoch: 545, Loss: 0.2047\n",
            "Epoch: 546, Loss: 0.2044\n",
            "Epoch: 547, Loss: 0.2042\n",
            "Epoch: 548, Loss: 0.2039\n",
            "Epoch: 549, Loss: 0.2036\n",
            "Epoch: 550, Loss: 0.2033\n",
            "Epoch: 551, Loss: 0.2031\n",
            "Epoch: 552, Loss: 0.2028\n",
            "Epoch: 553, Loss: 0.2025\n",
            "Epoch: 554, Loss: 0.2023\n",
            "Epoch: 555, Loss: 0.2020\n",
            "Epoch: 556, Loss: 0.2017\n",
            "Epoch: 557, Loss: 0.2015\n",
            "Epoch: 558, Loss: 0.2012\n",
            "Epoch: 559, Loss: 0.2009\n",
            "Epoch: 560, Loss: 0.2007\n",
            "Epoch: 561, Loss: 0.2004\n",
            "Epoch: 562, Loss: 0.2002\n",
            "Epoch: 563, Loss: 0.1999\n",
            "Epoch: 564, Loss: 0.1996\n",
            "Epoch: 565, Loss: 0.1994\n",
            "Epoch: 566, Loss: 0.1991\n",
            "Epoch: 567, Loss: 0.1989\n",
            "Epoch: 568, Loss: 0.1986\n",
            "Epoch: 569, Loss: 0.1983\n",
            "Epoch: 570, Loss: 0.1981\n",
            "Epoch: 571, Loss: 0.1978\n",
            "Epoch: 572, Loss: 0.1976\n",
            "Epoch: 573, Loss: 0.1973\n",
            "Epoch: 574, Loss: 0.1971\n",
            "Epoch: 575, Loss: 0.1968\n",
            "Epoch: 576, Loss: 0.1966\n",
            "Epoch: 577, Loss: 0.1963\n",
            "Epoch: 578, Loss: 0.1961\n",
            "Epoch: 579, Loss: 0.1958\n",
            "Epoch: 580, Loss: 0.1956\n",
            "Epoch: 581, Loss: 0.1953\n",
            "Epoch: 582, Loss: 0.1951\n",
            "Epoch: 583, Loss: 0.1948\n",
            "Epoch: 584, Loss: 0.1946\n",
            "Epoch: 585, Loss: 0.1943\n",
            "Epoch: 586, Loss: 0.1941\n",
            "Epoch: 587, Loss: 0.1938\n",
            "Epoch: 588, Loss: 0.1936\n",
            "Epoch: 589, Loss: 0.1933\n",
            "Epoch: 590, Loss: 0.1931\n",
            "Epoch: 591, Loss: 0.1929\n",
            "Epoch: 592, Loss: 0.1926\n",
            "Epoch: 593, Loss: 0.1924\n",
            "Epoch: 594, Loss: 0.1921\n",
            "Epoch: 595, Loss: 0.1919\n",
            "Epoch: 596, Loss: 0.1916\n",
            "Epoch: 597, Loss: 0.1914\n",
            "Epoch: 598, Loss: 0.1912\n",
            "Epoch: 599, Loss: 0.1909\n",
            "Epoch: 600, Loss: 0.1907\n",
            "Epoch: 601, Loss: 0.1905\n",
            "Epoch: 602, Loss: 0.1902\n",
            "Epoch: 603, Loss: 0.1900\n",
            "Epoch: 604, Loss: 0.1898\n",
            "Epoch: 605, Loss: 0.1895\n",
            "Epoch: 606, Loss: 0.1893\n",
            "Epoch: 607, Loss: 0.1891\n",
            "Epoch: 608, Loss: 0.1888\n",
            "Epoch: 609, Loss: 0.1886\n",
            "Epoch: 610, Loss: 0.1884\n",
            "Epoch: 611, Loss: 0.1881\n",
            "Epoch: 612, Loss: 0.1879\n",
            "Epoch: 613, Loss: 0.1877\n",
            "Epoch: 614, Loss: 0.1874\n",
            "Epoch: 615, Loss: 0.1872\n",
            "Epoch: 616, Loss: 0.1870\n",
            "Epoch: 617, Loss: 0.1868\n",
            "Epoch: 618, Loss: 0.1865\n",
            "Epoch: 619, Loss: 0.1863\n",
            "Epoch: 620, Loss: 0.1861\n",
            "Epoch: 621, Loss: 0.1859\n",
            "Epoch: 622, Loss: 0.1856\n",
            "Epoch: 623, Loss: 0.1854\n",
            "Epoch: 624, Loss: 0.1852\n",
            "Epoch: 625, Loss: 0.1850\n",
            "Epoch: 626, Loss: 0.1847\n",
            "Epoch: 627, Loss: 0.1845\n",
            "Epoch: 628, Loss: 0.1843\n",
            "Epoch: 629, Loss: 0.1841\n",
            "Epoch: 630, Loss: 0.1839\n",
            "Epoch: 631, Loss: 0.1836\n",
            "Epoch: 632, Loss: 0.1834\n",
            "Epoch: 633, Loss: 0.1832\n",
            "Epoch: 634, Loss: 0.1830\n",
            "Epoch: 635, Loss: 0.1828\n",
            "Epoch: 636, Loss: 0.1826\n",
            "Epoch: 637, Loss: 0.1823\n",
            "Epoch: 638, Loss: 0.1821\n",
            "Epoch: 639, Loss: 0.1819\n",
            "Epoch: 640, Loss: 0.1817\n",
            "Epoch: 641, Loss: 0.1815\n",
            "Epoch: 642, Loss: 0.1813\n",
            "Epoch: 643, Loss: 0.1811\n",
            "Epoch: 644, Loss: 0.1808\n",
            "Epoch: 645, Loss: 0.1806\n",
            "Epoch: 646, Loss: 0.1804\n",
            "Epoch: 647, Loss: 0.1802\n",
            "Epoch: 648, Loss: 0.1800\n",
            "Epoch: 649, Loss: 0.1798\n",
            "Epoch: 650, Loss: 0.1796\n",
            "Epoch: 651, Loss: 0.1794\n",
            "Epoch: 652, Loss: 0.1792\n",
            "Epoch: 653, Loss: 0.1790\n",
            "Epoch: 654, Loss: 0.1788\n",
            "Epoch: 655, Loss: 0.1786\n",
            "Epoch: 656, Loss: 0.1783\n",
            "Epoch: 657, Loss: 0.1781\n",
            "Epoch: 658, Loss: 0.1779\n",
            "Epoch: 659, Loss: 0.1777\n",
            "Epoch: 660, Loss: 0.1775\n",
            "Epoch: 661, Loss: 0.1773\n",
            "Epoch: 662, Loss: 0.1771\n",
            "Epoch: 663, Loss: 0.1769\n",
            "Epoch: 664, Loss: 0.1767\n",
            "Epoch: 665, Loss: 0.1765\n",
            "Epoch: 666, Loss: 0.1763\n",
            "Epoch: 667, Loss: 0.1761\n",
            "Epoch: 668, Loss: 0.1759\n",
            "Epoch: 669, Loss: 0.1757\n",
            "Epoch: 670, Loss: 0.1755\n",
            "Epoch: 671, Loss: 0.1753\n",
            "Epoch: 672, Loss: 0.1751\n",
            "Epoch: 673, Loss: 0.1749\n",
            "Epoch: 674, Loss: 0.1747\n",
            "Epoch: 675, Loss: 0.1746\n",
            "Epoch: 676, Loss: 0.1744\n",
            "Epoch: 677, Loss: 0.1742\n",
            "Epoch: 678, Loss: 0.1740\n",
            "Epoch: 679, Loss: 0.1738\n",
            "Epoch: 680, Loss: 0.1736\n",
            "Epoch: 681, Loss: 0.1734\n",
            "Epoch: 682, Loss: 0.1732\n",
            "Epoch: 683, Loss: 0.1730\n",
            "Epoch: 684, Loss: 0.1728\n",
            "Epoch: 685, Loss: 0.1726\n",
            "Epoch: 686, Loss: 0.1724\n",
            "Epoch: 687, Loss: 0.1722\n",
            "Epoch: 688, Loss: 0.1721\n",
            "Epoch: 689, Loss: 0.1719\n",
            "Epoch: 690, Loss: 0.1717\n",
            "Epoch: 691, Loss: 0.1715\n",
            "Epoch: 692, Loss: 0.1713\n",
            "Epoch: 693, Loss: 0.1711\n",
            "Epoch: 694, Loss: 0.1709\n",
            "Epoch: 695, Loss: 0.1708\n",
            "Epoch: 696, Loss: 0.1706\n",
            "Epoch: 697, Loss: 0.1704\n",
            "Epoch: 698, Loss: 0.1702\n",
            "Epoch: 699, Loss: 0.1700\n",
            "Epoch: 700, Loss: 0.1698\n",
            "Epoch: 701, Loss: 0.1696\n",
            "Epoch: 702, Loss: 0.1695\n",
            "Epoch: 703, Loss: 0.1693\n",
            "Epoch: 704, Loss: 0.1691\n",
            "Epoch: 705, Loss: 0.1689\n",
            "Epoch: 706, Loss: 0.1687\n",
            "Epoch: 707, Loss: 0.1686\n",
            "Epoch: 708, Loss: 0.1684\n",
            "Epoch: 709, Loss: 0.1682\n",
            "Epoch: 710, Loss: 0.1680\n",
            "Epoch: 711, Loss: 0.1678\n",
            "Epoch: 712, Loss: 0.1677\n",
            "Epoch: 713, Loss: 0.1675\n",
            "Epoch: 714, Loss: 0.1673\n",
            "Epoch: 715, Loss: 0.1671\n",
            "Epoch: 716, Loss: 0.1670\n",
            "Epoch: 717, Loss: 0.1668\n",
            "Epoch: 718, Loss: 0.1666\n",
            "Epoch: 719, Loss: 0.1664\n",
            "Epoch: 720, Loss: 0.1663\n",
            "Epoch: 721, Loss: 0.1661\n",
            "Epoch: 722, Loss: 0.1659\n",
            "Epoch: 723, Loss: 0.1658\n",
            "Epoch: 724, Loss: 0.1656\n",
            "Epoch: 725, Loss: 0.1654\n",
            "Epoch: 726, Loss: 0.1652\n",
            "Epoch: 727, Loss: 0.1651\n",
            "Epoch: 728, Loss: 0.1649\n",
            "Epoch: 729, Loss: 0.1647\n",
            "Epoch: 730, Loss: 0.1646\n",
            "Epoch: 731, Loss: 0.1644\n",
            "Epoch: 732, Loss: 0.1642\n",
            "Epoch: 733, Loss: 0.1641\n",
            "Epoch: 734, Loss: 0.1639\n",
            "Epoch: 735, Loss: 0.1637\n",
            "Epoch: 736, Loss: 0.1635\n",
            "Epoch: 737, Loss: 0.1634\n",
            "Epoch: 738, Loss: 0.1632\n",
            "Epoch: 739, Loss: 0.1631\n",
            "Epoch: 740, Loss: 0.1629\n",
            "Epoch: 741, Loss: 0.1627\n",
            "Epoch: 742, Loss: 0.1626\n",
            "Epoch: 743, Loss: 0.1624\n",
            "Epoch: 744, Loss: 0.1622\n",
            "Epoch: 745, Loss: 0.1621\n",
            "Epoch: 746, Loss: 0.1619\n",
            "Epoch: 747, Loss: 0.1617\n",
            "Epoch: 748, Loss: 0.1616\n",
            "Epoch: 749, Loss: 0.1614\n",
            "Epoch: 750, Loss: 0.1613\n",
            "Epoch: 751, Loss: 0.1611\n",
            "Epoch: 752, Loss: 0.1609\n",
            "Epoch: 753, Loss: 0.1608\n",
            "Epoch: 754, Loss: 0.1606\n",
            "Epoch: 755, Loss: 0.1605\n",
            "Epoch: 756, Loss: 0.1603\n",
            "Epoch: 757, Loss: 0.1601\n",
            "Epoch: 758, Loss: 0.1600\n",
            "Epoch: 759, Loss: 0.1598\n",
            "Epoch: 760, Loss: 0.1597\n",
            "Epoch: 761, Loss: 0.1595\n",
            "Epoch: 762, Loss: 0.1594\n",
            "Epoch: 763, Loss: 0.1592\n",
            "Epoch: 764, Loss: 0.1591\n",
            "Epoch: 765, Loss: 0.1589\n",
            "Epoch: 766, Loss: 0.1587\n",
            "Epoch: 767, Loss: 0.1586\n",
            "Epoch: 768, Loss: 0.1584\n",
            "Epoch: 769, Loss: 0.1583\n",
            "Epoch: 770, Loss: 0.1581\n",
            "Epoch: 771, Loss: 0.1580\n",
            "Epoch: 772, Loss: 0.1578\n",
            "Epoch: 773, Loss: 0.1577\n",
            "Epoch: 774, Loss: 0.1575\n",
            "Epoch: 775, Loss: 0.1574\n",
            "Epoch: 776, Loss: 0.1572\n",
            "Epoch: 777, Loss: 0.1571\n",
            "Epoch: 778, Loss: 0.1569\n",
            "Epoch: 779, Loss: 0.1568\n",
            "Epoch: 780, Loss: 0.1566\n",
            "Epoch: 781, Loss: 0.1565\n",
            "Epoch: 782, Loss: 0.1563\n",
            "Epoch: 783, Loss: 0.1562\n",
            "Epoch: 784, Loss: 0.1560\n",
            "Epoch: 785, Loss: 0.1559\n",
            "Epoch: 786, Loss: 0.1557\n",
            "Epoch: 787, Loss: 0.1556\n",
            "Epoch: 788, Loss: 0.1554\n",
            "Epoch: 789, Loss: 0.1553\n",
            "Epoch: 790, Loss: 0.1552\n",
            "Epoch: 791, Loss: 0.1550\n",
            "Epoch: 792, Loss: 0.1549\n",
            "Epoch: 793, Loss: 0.1547\n",
            "Epoch: 794, Loss: 0.1546\n",
            "Epoch: 795, Loss: 0.1544\n",
            "Epoch: 796, Loss: 0.1543\n",
            "Epoch: 797, Loss: 0.1542\n",
            "Epoch: 798, Loss: 0.1540\n",
            "Epoch: 799, Loss: 0.1539\n",
            "Epoch: 800, Loss: 0.1537\n",
            "Epoch: 801, Loss: 0.1536\n",
            "Epoch: 802, Loss: 0.1534\n",
            "Epoch: 803, Loss: 0.1533\n",
            "Epoch: 804, Loss: 0.1532\n",
            "Epoch: 805, Loss: 0.1530\n",
            "Epoch: 806, Loss: 0.1529\n",
            "Epoch: 807, Loss: 0.1528\n",
            "Epoch: 808, Loss: 0.1526\n",
            "Epoch: 809, Loss: 0.1525\n",
            "Epoch: 810, Loss: 0.1523\n",
            "Epoch: 811, Loss: 0.1522\n",
            "Epoch: 812, Loss: 0.1521\n",
            "Epoch: 813, Loss: 0.1519\n",
            "Epoch: 814, Loss: 0.1518\n",
            "Epoch: 815, Loss: 0.1517\n",
            "Epoch: 816, Loss: 0.1515\n",
            "Epoch: 817, Loss: 0.1514\n",
            "Epoch: 818, Loss: 0.1512\n",
            "Epoch: 819, Loss: 0.1511\n",
            "Epoch: 820, Loss: 0.1510\n",
            "Epoch: 821, Loss: 0.1508\n",
            "Epoch: 822, Loss: 0.1507\n",
            "Epoch: 823, Loss: 0.1506\n",
            "Epoch: 824, Loss: 0.1504\n",
            "Epoch: 825, Loss: 0.1503\n",
            "Epoch: 826, Loss: 0.1502\n",
            "Epoch: 827, Loss: 0.1501\n",
            "Epoch: 828, Loss: 0.1499\n",
            "Epoch: 829, Loss: 0.1498\n",
            "Epoch: 830, Loss: 0.1497\n",
            "Epoch: 831, Loss: 0.1495\n",
            "Epoch: 832, Loss: 0.1494\n",
            "Epoch: 833, Loss: 0.1493\n",
            "Epoch: 834, Loss: 0.1491\n",
            "Epoch: 835, Loss: 0.1490\n",
            "Epoch: 836, Loss: 0.1489\n",
            "Epoch: 837, Loss: 0.1488\n",
            "Epoch: 838, Loss: 0.1486\n",
            "Epoch: 839, Loss: 0.1485\n",
            "Epoch: 840, Loss: 0.1484\n",
            "Epoch: 841, Loss: 0.1482\n",
            "Epoch: 842, Loss: 0.1481\n",
            "Epoch: 843, Loss: 0.1480\n",
            "Epoch: 844, Loss: 0.1479\n",
            "Epoch: 845, Loss: 0.1477\n",
            "Epoch: 846, Loss: 0.1476\n",
            "Epoch: 847, Loss: 0.1475\n",
            "Epoch: 848, Loss: 0.1474\n",
            "Epoch: 849, Loss: 0.1472\n",
            "Epoch: 850, Loss: 0.1471\n",
            "Epoch: 851, Loss: 0.1470\n",
            "Epoch: 852, Loss: 0.1469\n",
            "Epoch: 853, Loss: 0.1467\n",
            "Epoch: 854, Loss: 0.1466\n",
            "Epoch: 855, Loss: 0.1465\n",
            "Epoch: 856, Loss: 0.1464\n",
            "Epoch: 857, Loss: 0.1463\n",
            "Epoch: 858, Loss: 0.1461\n",
            "Epoch: 859, Loss: 0.1460\n",
            "Epoch: 860, Loss: 0.1459\n",
            "Epoch: 861, Loss: 0.1458\n",
            "Epoch: 862, Loss: 0.1456\n",
            "Epoch: 863, Loss: 0.1455\n",
            "Epoch: 864, Loss: 0.1454\n",
            "Epoch: 865, Loss: 0.1453\n",
            "Epoch: 866, Loss: 0.1452\n",
            "Epoch: 867, Loss: 0.1451\n",
            "Epoch: 868, Loss: 0.1449\n",
            "Epoch: 869, Loss: 0.1448\n",
            "Epoch: 870, Loss: 0.1447\n",
            "Epoch: 871, Loss: 0.1446\n",
            "Epoch: 872, Loss: 0.1445\n",
            "Epoch: 873, Loss: 0.1443\n",
            "Epoch: 874, Loss: 0.1442\n",
            "Epoch: 875, Loss: 0.1441\n",
            "Epoch: 876, Loss: 0.1440\n",
            "Epoch: 877, Loss: 0.1439\n",
            "Epoch: 878, Loss: 0.1438\n",
            "Epoch: 879, Loss: 0.1437\n",
            "Epoch: 880, Loss: 0.1435\n",
            "Epoch: 881, Loss: 0.1434\n",
            "Epoch: 882, Loss: 0.1433\n",
            "Epoch: 883, Loss: 0.1432\n",
            "Epoch: 884, Loss: 0.1431\n",
            "Epoch: 885, Loss: 0.1430\n",
            "Epoch: 886, Loss: 0.1429\n",
            "Epoch: 887, Loss: 0.1427\n",
            "Epoch: 888, Loss: 0.1426\n",
            "Epoch: 889, Loss: 0.1425\n",
            "Epoch: 890, Loss: 0.1424\n",
            "Epoch: 891, Loss: 0.1423\n",
            "Epoch: 892, Loss: 0.1422\n",
            "Epoch: 893, Loss: 0.1421\n",
            "Epoch: 894, Loss: 0.1420\n",
            "Epoch: 895, Loss: 0.1418\n",
            "Epoch: 896, Loss: 0.1417\n",
            "Epoch: 897, Loss: 0.1416\n",
            "Epoch: 898, Loss: 0.1415\n",
            "Epoch: 899, Loss: 0.1414\n",
            "Epoch: 900, Loss: 0.1413\n",
            "Epoch: 901, Loss: 0.1412\n",
            "Epoch: 902, Loss: 0.1411\n",
            "Epoch: 903, Loss: 0.1410\n",
            "Epoch: 904, Loss: 0.1409\n",
            "Epoch: 905, Loss: 0.1408\n",
            "Epoch: 906, Loss: 0.1407\n",
            "Epoch: 907, Loss: 0.1405\n",
            "Epoch: 908, Loss: 0.1404\n",
            "Epoch: 909, Loss: 0.1403\n",
            "Epoch: 910, Loss: 0.1402\n",
            "Epoch: 911, Loss: 0.1401\n",
            "Epoch: 912, Loss: 0.1400\n",
            "Epoch: 913, Loss: 0.1399\n",
            "Epoch: 914, Loss: 0.1398\n",
            "Epoch: 915, Loss: 0.1397\n",
            "Epoch: 916, Loss: 0.1396\n",
            "Epoch: 917, Loss: 0.1395\n",
            "Epoch: 918, Loss: 0.1394\n",
            "Epoch: 919, Loss: 0.1393\n",
            "Epoch: 920, Loss: 0.1392\n",
            "Epoch: 921, Loss: 0.1391\n",
            "Epoch: 922, Loss: 0.1390\n",
            "Epoch: 923, Loss: 0.1389\n",
            "Epoch: 924, Loss: 0.1388\n",
            "Epoch: 925, Loss: 0.1387\n",
            "Epoch: 926, Loss: 0.1386\n",
            "Epoch: 927, Loss: 0.1385\n",
            "Epoch: 928, Loss: 0.1384\n",
            "Epoch: 929, Loss: 0.1383\n",
            "Epoch: 930, Loss: 0.1382\n",
            "Epoch: 931, Loss: 0.1381\n",
            "Epoch: 932, Loss: 0.1380\n",
            "Epoch: 933, Loss: 0.1379\n",
            "Epoch: 934, Loss: 0.1378\n",
            "Epoch: 935, Loss: 0.1377\n",
            "Epoch: 936, Loss: 0.1376\n",
            "Epoch: 937, Loss: 0.1375\n",
            "Epoch: 938, Loss: 0.1374\n",
            "Epoch: 939, Loss: 0.1373\n",
            "Epoch: 940, Loss: 0.1372\n",
            "Epoch: 941, Loss: 0.1371\n",
            "Epoch: 942, Loss: 0.1370\n",
            "Epoch: 943, Loss: 0.1369\n",
            "Epoch: 944, Loss: 0.1368\n",
            "Epoch: 945, Loss: 0.1367\n",
            "Epoch: 946, Loss: 0.1366\n",
            "Epoch: 947, Loss: 0.1365\n",
            "Epoch: 948, Loss: 0.1364\n",
            "Epoch: 949, Loss: 0.1363\n",
            "Epoch: 950, Loss: 0.1362\n",
            "Epoch: 951, Loss: 0.1361\n",
            "Epoch: 952, Loss: 0.1360\n",
            "Epoch: 953, Loss: 0.1359\n",
            "Epoch: 954, Loss: 0.1358\n",
            "Epoch: 955, Loss: 0.1357\n",
            "Epoch: 956, Loss: 0.1356\n",
            "Epoch: 957, Loss: 0.1355\n",
            "Epoch: 958, Loss: 0.1354\n",
            "Epoch: 959, Loss: 0.1353\n",
            "Epoch: 960, Loss: 0.1352\n",
            "Epoch: 961, Loss: 0.1352\n",
            "Epoch: 962, Loss: 0.1351\n",
            "Epoch: 963, Loss: 0.1350\n",
            "Epoch: 964, Loss: 0.1349\n",
            "Epoch: 965, Loss: 0.1348\n",
            "Epoch: 966, Loss: 0.1347\n",
            "Epoch: 967, Loss: 0.1346\n",
            "Epoch: 968, Loss: 0.1345\n",
            "Epoch: 969, Loss: 0.1344\n",
            "Epoch: 970, Loss: 0.1343\n",
            "Epoch: 971, Loss: 0.1342\n",
            "Epoch: 972, Loss: 0.1341\n",
            "Epoch: 973, Loss: 0.1341\n",
            "Epoch: 974, Loss: 0.1340\n",
            "Epoch: 975, Loss: 0.1339\n",
            "Epoch: 976, Loss: 0.1338\n",
            "Epoch: 977, Loss: 0.1337\n",
            "Epoch: 978, Loss: 0.1336\n",
            "Epoch: 979, Loss: 0.1335\n",
            "Epoch: 980, Loss: 0.1334\n",
            "Epoch: 981, Loss: 0.1333\n",
            "Epoch: 982, Loss: 0.1333\n",
            "Epoch: 983, Loss: 0.1332\n",
            "Epoch: 984, Loss: 0.1331\n",
            "Epoch: 985, Loss: 0.1330\n",
            "Epoch: 986, Loss: 0.1329\n",
            "Epoch: 987, Loss: 0.1328\n",
            "Epoch: 988, Loss: 0.1327\n",
            "Epoch: 989, Loss: 0.1326\n",
            "Epoch: 990, Loss: 0.1326\n",
            "Epoch: 991, Loss: 0.1325\n",
            "Epoch: 992, Loss: 0.1324\n",
            "Epoch: 993, Loss: 0.1323\n",
            "Epoch: 994, Loss: 0.1322\n",
            "Epoch: 995, Loss: 0.1321\n",
            "Epoch: 996, Loss: 0.1320\n",
            "Epoch: 997, Loss: 0.1320\n",
            "Epoch: 998, Loss: 0.1319\n",
            "Epoch: 999, Loss: 0.1318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X5wWovWpN1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acf1674f-358b-45c3-8b58-2e3843248faa"
      },
      "source": [
        "for param in model.parameters():\n",
        "  print(param.data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3.2174, 4.5916, 1.9581]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}